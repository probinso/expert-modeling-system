@inproceedings{Rosen-Zvi2004,
 author = {Rosen-Zvi, Michal and Griffiths, Thomas and Steyvers, Mark and Smyth, Padhraic},
 title = {The Author-topic Model for Authors and Documents},
 booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
 series = {UAI '04},
 year = {2004},
 isbn = {0-9749039-0-6},
 location = {Banff, Canada},
 pages = {487--494},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1036843.1036902},
 acmid = {1036902},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
}
@incollection{NIPS2005_2906,
title = {Correlated Topic Models},
author = {John D. Lafferty and David M. Blei},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
pages = {147--154},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2906-correlated-topic-models.pdf}
}
@article{Alghamdi2015,
year = {2015},
author = {Rubayyi Alghamdi and Khalid Alfalqi},
title = {A Survey of Topic Modeling in Text Mining},
journal = {International Journal of Advanced Computer Science and Applications},
doi = {10.14569/IJACSA.2015.060121},
url = {http://dx.doi.org/10.14569/IJACSA.2015.060121},
publisher = {The Science and Information Organization},
volume = {6},
number = {1},
}
@inbook{Salomatin2009MultifieldCT,
author = {Konstantin Salomatin and Yiming Yang and Abhimanyu Lad},
title = {Multi-field Correlated Topic Modeling},
booktitle = {Proceedings of the 2009 SIAM International Conference on Data Mining},
chapter = {},
pages = {628-637},
doi = {10.1137/1.9781611972795.54},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972795.54},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611972795.54}
}
@Article{Rexha2018,
author="Rexha, Andi
and Kr{\"o}ll, Mark
and Ziak, Hermann
and Kern, Roman",
title="Authorship identification of documents with high content similarity",
journal="Scientometrics",
year="2018",
month="Apr",
day="01",
volume="115",
number="1",
pages="223--237",
abstract="The goal of our work is inspired by the task of associating segments of text to their real authors. In this work, we focus on analyzing the way humans judge different writing styles. This analysis can help to better understand this process and to thus simulate/ mimic such behavior accordingly. Unlike the majority of the work done in this field (i.e. authorship attribution, plagiarism detection, etc.) which uses content features, we focus only on the stylometric, i.e. content-agnostic, characteristics of authors. Therefore, we conducted two pilot studies to determine, if humans can identify authorship among documents with high content similarity. The first was a quantitative experiment involving crowd-sourcing, while the second was a qualitative one executed by the authors of this paper. Both studies confirmed that this task is quite challenging. To gain a better understanding of how humans tackle such a problem, we conducted an exploratory data analysis on the results of the studies. In the first experiment, we compared the decisions against content features and stylometric features. While in the second, the evaluators described the process and the features on which their judgment was based. The findings of our detailed analysis could (1) help to improve algorithms such as automatic authorship attribution as well as plagiarism detection, (2) assist forensic experts or linguists to create profiles of writers, (3) support intelligence applications to analyze aggressive and threatening messages and (4) help editor conformity by adhering to, for instance, journal specific writing style.",
issn="1588-2861",
doi="10.1007/s11192-018-2661-6",
url="https://doi.org/10.1007/s11192-018-2661-6"
}
@inproceedings{Minto2007,
 author = {Minto, Shawn and Murphy, Gail C.},
 title = {Recommending Emergent Teams},
 booktitle = {Proceedings of the Fourth International Workshop on Mining Software Repositories},
 series = {MSR '07},
 year = {2007},
 isbn = {0-7695-2950-X},
 pages = {5--},
 url = {https://doi.org/10.1109/MSR.2007.27},
 doi = {10.1109/MSR.2007.27},
 acmid = {1269021},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}
@inproceedings{Yan2013,
 author = {Yan, Xiaohui and Guo, Jiafeng and Lan, Yanyan and Cheng, Xueqi},
 title = {A Biterm Topic Model for Short Texts},
 booktitle = {Proceedings of the 22Nd International Conference on World Wide Web},
 series = {WWW '13},
 year = {2013},
 isbn = {978-1-4503-2035-1},
 location = {Rio de Janeiro, Brazil},
 pages = {1445--1456},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2488388.2488514},
 doi = {10.1145/2488388.2488514},
 acmid = {2488514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {biterm, content analysis, short text, topic model},
}
@inproceedings{AldebeiHJ016,
  author    = {Khaled Aldebei and
               Xiangjian He and
               Wenjing Jia and
               Jie Yang},
  title     = {Unsupervised Multi-Author Document Decomposition Based on Hidden Markov
               Model},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume
               1: Long Papers},
  year      = {2016},
  url       = {http://aclweb.org/anthology/P/P16/P16-1067.pdf},
  timestamp = {Mon, 15 Aug 2016 15:53:28 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/AldebeiHJ016},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{kiran2010,
author="G.V.R., Kiran
and Shankar, Ravi
and Pudi, Vikram",
editor="Setchi, Rossitza
and Jordanov, Ivan
and Howlett, Robert J.
and Jain, Lakhmi C.",
title="Frequent Itemset Based Hierarchical Document Clustering Using Wikipedia as External Knowledge",
booktitle="Knowledge-Based and Intelligent Information and Engineering Systems",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="11--20",
abstract="High dimensionality is a major challenge in document clustering. Some of the recent algorithms address this problem by using frequent itemsets for clustering. But, most of these algorithms neglect the semantic relationship between the words. On the other hand there are algorithms that take care of the semantic relations between the words by making use of external knowledge contained in WordNet, Mesh, Wikipedia, etc but do not handle the high dimensionality. In this paper we present an efficient solution that addresses both these problems. We propose a hierarchical clustering algorithm using closed frequent itemsets that use Wikipedia as an external knowledge to enhance the document representation. We evaluate our methods based on F-Score on standard datasets and show our results to be better than existing approaches.",
isbn="978-3-642-15390-7"
}
@article{wang2003,
author = {C. M, Benjamin and Ke, Fung and Martin Ester, Wang},
year = {2003},
month = {04},
pages = {},
title = {Hierarchical Document Clustering Using Frequent Itemsets}
}
@article{Agrawal1993,
author = {Agrawal, Rakesh and Imielinski, T and Swami, Arun},
year = {1993},
month = {01},
pages = {},
title = {Mining Associations Between Sets of Items in Massive Databases},
booktitle = {Proceedings of the ACM-SIGMOD 1993 International Conference on Management of Data, 1993}
}
@inproceedings{rehurek_lrec,
title = {{Software Framework for Topic Modelling with Large Corpora}},
author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
booktitle = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
pages = {45--50},
year = 2010,
month = May,
day = 22,
publisher = {ELRA},
address = {Valletta, Malta},
note={\url{http://is.muni.cz/publication/884893/en}},
language={English}
}
@article{Feige2002,
author = {Feige, Uriel and Krauthgamer, Robert},
title = {A Polylogarithmic Approximation of the Minimum Bisection},
journal = {SIAM J. Comput.},
issue_date = {2002},
volume = {31},
number = {4},
month = apr,
year = {2002},
issn = {0097-5397},
pages = {1090--1118},
numpages = {29},
url = {http://dx.doi.org/10.1137/S0097539701387660},
doi = {10.1137/S0097539701387660},
acmid = {586910},
publisher = {Society for Industrial and Applied Mathematics},
address = {Philadelphia, PA, USA},
keywords = {approximation algorithms, divide and conquer, dynamic programming, graph partitioning, graph separators},
}
@article{TACL582,
        author = {Nguyen, Dat Quoc  and Billingsley, Richard  and Du, Lan  and Johnson, Mark },
        title = {Improving Topic Models with Latent Feature Word Representations},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks. In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.},
        issn = {2307-387X},
        url = {https://transacl.org/ojs/index.php/tacl/article/view/582},
        pages = {299--313}
}
@inproceedings{Chen2015,
  title={User Based Aggregation for Biterm Topic Model},
  author={Weizheng Chen and Jinpeng Wang and Yan Zhang and Hongfei Yan and Xiaoming Li},
  booktitle={ACL},
  year={2015}
}
@article{LimCB16,
  author    = {Kar Wai Lim and
               Changyou Chen and
               Wray L. Buntine},
  title     = {Twitter-Network Topic Model: {A} Full Bayesian Treatment for Social
               Network and Text Modeling},
  journal   = {CoRR},
  volume    = {abs/1609.06791},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.06791},
  archivePrefix = {arXiv},
  eprint    = {1609.06791},
  timestamp = {Wed, 07 Jun 2017 14:41:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LimCB16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{lambert2018,
  title={A Student’s Guide to Bayesian Statistics},
  author={Lambert, B.},
  isbn={9781526418265},
  url={https://books.google.com/books?id=CLZBDwAAQBAJ},
  year={2018},
  publisher={SAGE Publications}
}
@unknown{Zhao2016,
author = {Zhao, Yukun and Liang, Shangsong and Ren, Zhaochun and Ma, Jun and Yilmaz, Emine and de Rijke, Maarten},
year = {2016},
month = {07},
pages = {155-164},
title = {Explainable User Clustering in Short Text Streams}
}
@article{doi:10.1093/llc/fqt066,
author = {Eder, Maciej},
title = {Does size matter? Authorship attribution, small samples, big problem},
journal = {Digital Scholarship in the Humanities},
volume = {30},
number = {2},
pages = {167-182},
year = {2015},
doi = {10.1093/llc/fqt066},
URL = {http://dx.doi.org/10.1093/llc/fqt066},
eprint = {/oup/backfile/content_public/journal/dsh/30/2/10.1093_llc_fqt066/4/fqt066.pdf},
abstract = {The aim of this study is to find such a minimal size of text samples for authorship attribution that would provide stable results independent of random noise. A few controlled tests for different sample lengths, languages, and genres are discussed and compared. Depending on the corpus used, the minimal sample length varied from 2,500 words (Latin prose) to 5,000 or so words (in most cases, including English, German, Polish, and Hungarian novels). Another observation is connected with the method of sampling: contrary to common sense, randomly excerpted ‘bags of words’ turned out to be much more effective than the classical solution, i.e. using original sequences of words (‘passages’) of desired size. Although the tests have been performed using the Delta method (Burrows, J.F. (2002). ‘Delta’: a measure of stylistic difference and a guide to likely authorship. Literary and Linguistic Computing, 17(3): 267–87) applied to the most frequent words, some additional experiments have been conducted for support vector machines and k-NN applied to most frequent words, character 3-grams, character 4-grams, and parts-of-speech-tag 3-grams. Despite significant differences in overall attributive success rate between particular methods and/or style markers, the minimal amount of textual data needed for reliable authorship attribution turned out to be method-independent.}
}
@article{Yang2016,
 author = {Yang, Yi and Pan, Shimei and Lu, Jie and Topkara, Mercan and Song, Yangqiu},
 title = {The Stability and Usability of Statistical Topic Models},
 journal = {ACM Trans. Interact. Intell. Syst.},
 issue_date = {August 2016},
 volume = {6},
 number = {2},
 month = jul,
 year = {2016},
 issn = {2160-6455},
 pages = {14:1--14:23},
 articleno = {14},
 numpages = {23},
 url = {http://doi.acm.org/10.1145/2954002},
 doi = {10.1145/2954002},
 acmid = {2954002},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LDA, Statistical topic model, constrained topic model, non-disruptive topic model update, stability, text analytics, usability},
}
@article{Ren2013,
author = {Li Ren},
year = {2013},
title = {A Survey on Statistical Topic Modeling}
}
@article{Gunawardan,
 author = {Gunawardana, Asela and Shani, Guy},
 title = {A Survey of Accuracy Evaluation Metrics of Recommendation Tasks},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = dec,
 year = {2009},
 issn = {1532-4435},
 pages = {2935--2962},
 numpages = {28},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1755883},
 acmid = {1755883},
 publisher = {JMLR.org},
}
@Article{Silveira2017,
author="Silveira, Thiago
and Zhang, Min
and Lin, Xiao
and Liu, Yiqun
and Ma, Shaoping",
title="How good your recommender system is? A survey on evaluations in recommendation",
journal="International Journal of Machine Learning and Cybernetics",
year="2017",
month="Dec",
day="14",
abstract="Recommender Systems have become a very useful tool for a large variety of domains. Researchers have been attempting to improve their algorithms in order to issue better predictions to the users. However, one of the current challenges in the area refers to how to properly evaluate the predictions generated by a recommender system. In the extent of offline evaluations, some traditional concepts of evaluation have been explored, such as accuracy, Root Mean Square Error and P@N for top-k recommendations. In recent years, more research have proposed some new concepts such as novelty, diversity and serendipity. These concepts have been addressed with the goal to satisfy the users' requirements. Numerous definitions and metrics have been proposed in previous work. On the absence of a specific summarization on evaluations of recommendation combining traditional metrics and recent progresses, this paper surveys and organizes the main research that present definitions about concepts and propose metrics or strategies to evaluate recommendations. In addition, this survey also settles the relationship between the concepts, categorizes them according to their objectives and suggests potential future topics on user satisfaction.",
issn="1868-808X",
doi="10.1007/s13042-017-0762-9",
url="https://doi.org/10.1007/s13042-017-0762-9"
}
@inproceedings{Roder2015,
 author = {R\"{o}der, Michael and Both, Andreas and Hinneburg, Alexander},
 title = {Exploring the Space of Topic Coherence Measures},
 booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
 series = {WSDM '15},
 year = {2015},
 isbn = {978-1-4503-3317-7},
 location = {Shanghai, China},
 pages = {399--408},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2684822.2685324},
 doi = {10.1145/2684822.2685324},
 acmid = {2685324},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {topic coherence, topic evaluation, topic model},
}
@inproceedings{Hawking04,
  author = {David Hawking},
  title = {Challenges in Enterprise Search},
  booktitle = {Proceedings of the {Australasian Database Conference ADC2004}},
  year = {2004},
  location = {Dunedin, New Zealand},
  month = {January},
  pages = {15--26},
  note = {Invited paper: \url{http://david-hawking.net/pubs/hawking_adc04keynote.pdf}}
}
@inproceedings{sievert-shirley-2014-ldavis,
    title = "{LDA}vis: A method for visualizing and interpreting topics",
    author = "Sievert, Carson  and
      Shirley, Kenneth",
    booktitle = "Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3110",
    doi = "10.3115/v1/W14-3110",
    pages = "63--70",
}
@inproceedings{2012-termite,
  title = {Termite: Visualization Techniques for  Assessing Textual Topic Models},
  author = {Jason Chuang AND Christopher D. Manning AND Jeffrey Heer},
  booktitle = {Advanced Visual Interfaces},
  year = {2012},
  url = {http://vis.stanford.edu/papers/termite}
}
@article{DBLP:journals/corr/abs-1301-6705,
  author    = {Thomas Hofmann},
  title     = {Probabilistic Latent Semantic Analysis},
  journal   = {CoRR},
  volume    = {abs/1301.6705},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.6705},
  archivePrefix = {arXiv},
  eprint    = {1301.6705},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-6705.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{deerwester-indexing-1990,
  added-at = {2008-11-10T11:19:43.000+0100},
  author = {Deerwester, S. and Dumais, S.T. and Furnas, G.W. and Landauer, T.K. and Harshman, R.A.},
  biburl = {https://www.bibsonomy.org/bibtex/2e740ea9eb09bf939da47645ce2502c25/dnoack},
  interhash = {c15e0f019b2b967d224e7443100e8ff9},
  intrahash = {e740ea9eb09bf939da47645ce2502c25},
  journal = {Journal of the American Society for Information Science 41},
  keywords = {indexing semantic_analysis semantics wismasys0809},
  pages = {391-407},
  timestamp = {2008-11-10T11:20:53.000+0100},
  title = {Indexing by latent semantic analysis.},
  year = 1990
}
@article{BleiNg2010,
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  added-at = {2010-03-23T16:09:41.000+0100},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  biburl = {https://www.bibsonomy.org/bibtex/2bc34cc810fa7dfa12b949b60c23d9f5c/zhenzhenx},
  description = {Latent dirichlet allocation},
  doi = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
  interhash = {9d1b808272b9e511425cbf557571e59a},
  intrahash = {bc34cc810fa7dfa12b949b60c23d9f5c},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  keywords = {LDA allocation dirichlet latent},
  pages = {993--1022},
  publisher = {JMLR.org},
  timestamp = {2010-06-16T11:05:42.000+0200},
  title = {Latent dirichlet allocation},
  url = {http://portal.acm.org/citation.cfm?id=944937},
  volume = 3,
  year = 2003
}
@incollection{NIPS2007_3328,
title = {Supervised Topic Models},
author = {Jon D. Mcauliffe and David M. Blei},
booktitle = {Advances in Neural Information Processing Systems 20},
editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
pages = {121--128},
year = {2008},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf}
}
@article{Teh:EtAl:06,
  abstract = {We consider problems involving groups of data, where each observation within a group is a draw from a mixture model, and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture componentswithin each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the "Chinese restaurant franchise". We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures, and describe applications to problems in information retrieval and text modelling.},
  added-at = {2007-03-02T00:36:19.000+0100},
  author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
  biburl = {https://www.bibsonomy.org/bibtex/2306a104860208c7a3c9be306ef709008/seandalai},
  interhash = {34e30f6d1538ed136344f6a9cf8a791b},
  intrahash = {306a104860208c7a3c9be306ef709008},
  journal = {Journal of the American Statistical Association},
  keywords = {2006 dirichlet bayesian},
  number = 476,
  pages = {1566--1581},
  timestamp = {2007-03-02T00:36:19.000+0100},
  title = {Hierarchical Dirichlet Processes},
  url = {http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/jasa2006.pdf},
  volume = 101,
  year = 2006
}
@inproceedings{alsumait2009topic,
  author={Loulwah AlSumait and Daniel Barbará and James Gentle and Carlotta Domeniconi},
  title={Topic Significance Ranking of LDA Generative Models},
  booktitle={ECML},
  year={2009},
  url={http://www.springerlink.com/content/v3jth868647716kg/},
}
@InProceedings{krstovski2013efficient,
author = {Krstovski, Kriste and Smith, David A. and Wallach, Hanna and McGregor, Andrew},
title = {Efficient Nearest-Neighbor Search in the Probability Simplex},
booktitle = {2013 Conference on the Theory of Information Retrieval},
year = {2013},
month = {September},
abstract = {Document similarity tasks arise in many areas of information retrieval and natural language processing. A fundamental question when comparing documents is which representation to use. Topic models, which have served as versatile tools for exploratory data analysis and visualization, represent documents as probability distributions over latent topics. Systems comparing topic distributions thus use measures of probability divergence such as Kullback-Leibler, Jensen-Shannon, or Hellinger. This paper presents novel analysis and applications of the reduction of Hellinger divergence to Euclidean distance computations. This reduction allows us to exploit fast approximate nearest-neighbor (NN) techniques, such as locality-sensitive hashing (LSH) and approximate search in k-d trees, for search in the probability simplex. We demonstrate the effectiveness and efficiency of this approach on two tasks using latent Dirichlet allocation (LDA) document representations: discovering relationships between National Institutes of Health (NIH) grants and prior-art retrieval for patents. Evaluation on these tasks and on synthetic data shows that both Euclidean LSH and approximate k-d tree search perform well when a single nearest neighbor must be found. When a larger set of similar documents is to be retrieved, the k-d tree approach is more effective and efficient.},
url = {https://www.microsoft.com/en-us/research/publication/efficient-nearest-neighbor-search-in-the-probability-simplex/},
}