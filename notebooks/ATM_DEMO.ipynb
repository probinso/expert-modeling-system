{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, Counter\n",
    "from glob import glob\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from os import remove\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel as LDA\n",
    "from gensim.models import AuthorTopicModel as ATM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame, read_csv, concat, Series, set_option\n",
    "\n",
    "import pyLDAvis as ldavis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import coo_matrix as sparse_matrix\n",
    "\n",
    "ldavis.enable_notebook()\n",
    "%matplotlib notebook\n",
    "set_option('display.max_colwidth', -1)\n",
    "set_option('display.width', 100)\n",
    "#%precision 4\n",
    "\n",
    "\n",
    "Candidate = namedtuple('Candidate', ['iterations', 'num_topics'])\n",
    "\n",
    "\n",
    "\n",
    "def get_i_t(filename):\n",
    "    _, content, document_type = filename.split('-')\n",
    "    i, t, _ = content.split('_')\n",
    "    return int(i[1:]), int(t[1:]), document_type.split('.')[0]\n",
    "\n",
    "get_texts = lambda df: df[target].str.split()\n",
    "tobows = lambda df, d: concat([df['Anomaly_ID'], get_texts(df).apply(d.doc2bow)], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "isr2 = 2.0 ** -.5\n",
    "\n",
    "def hellinger(x, y):\n",
    "    return isr2 * np.sqrt(((np.sqrt(x) - np.sqrt(y)) ** 2).sum())\n",
    "\n",
    "\n",
    "\n",
    "report_types = 'ISA', 'PFR', 'DPFR'\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "min_occurances = 2\n",
    "target = 'GLOMUNSTEM'\n",
    "\n",
    "%matplotlib notebook\n",
    "%precision 4\n",
    "\n",
    "BASEDIR = Path('../data')\n",
    "OUT = Path('../output/atm/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'processed_authors.csv') as fd:\n",
    "    af = read_csv(fd)\n",
    "af.shape\n",
    "af.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_type = {\n",
    "    t: af[af.ReportType == t] \n",
    "    for t in af.ReportType.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type = {\n",
    "    t: read_csv(OUT / f'../norm_{t}.csv').dropna()\n",
    "    for t in af.ReportType.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_and_vocab(documents, test_size, min_occurances=min_occurances):\n",
    "    train, test = train_test_split(\n",
    "        documents, test_size=test_size\n",
    "    )\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    vocab = Dictionary(\n",
    "        train[target].str.split()\n",
    "    )\n",
    "    vocab.filter_extremes(no_below=min_occurances)\n",
    "\n",
    "    return train, test, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_document_downselect(documents, authors, ANOMALY_LABEL='Anomaly_ID'):\n",
    "    documents[ANOMALY_LABEL] = \\\n",
    "      'A' + documents[ANOMALY_LABEL].apply(str)\n",
    "\n",
    "    idx = documents[ANOMALY_LABEL].isin(\n",
    "        authors[ANOMALY_LABEL].unique()\n",
    "    )\n",
    "\n",
    "    documents = documents[idx]\n",
    "    \n",
    "    idx = authors[ANOMALY_LABEL].isin(\n",
    "        documents[ANOMALY_LABEL].unique()\n",
    "    )\n",
    "    authors = authors[idx]\n",
    "    return documents, authors\n",
    "\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    documents_by_type[document_type], authors_by_type[document_type] = \\\n",
    "        author_document_downselect(\n",
    "          documents_by_type[document_type],\n",
    "          authors_by_type[document_type]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_by_type, train_documents_by_type, test_documents_by_type, dictionary_by_type = \\\n",
    "  dict(), dict(), dict(), dict()\n",
    "\n",
    "train_author_table_by_type = dict()\n",
    "test_author_table_by_type = dict()\n",
    "\n",
    "def attribution_table(documents, relevent_authors):\n",
    "    store = defaultdict(set)\n",
    "    for idx, anomaly in documents.iterrows():\n",
    "        authors_documents = relevent_authors[\n",
    "            relevent_authors.Anomaly_ID == anomaly.Anomaly_ID\n",
    "        ]\n",
    "\n",
    "        for author in authors_documents.Users_ID:\n",
    "            store[author].add(idx)\n",
    "\n",
    "    return {k: list(v) for k, v in store.items()}\n",
    "\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    train_documents_by_type[document_type], \\\n",
    "    test_documents_by_type[document_type], \\\n",
    "    dictionary_by_type[document_type] = \\\n",
    "        get_train_test_and_vocab(documents_by_type[document_type], TEST_SIZE)\n",
    "\n",
    "    train_author_table_by_type[document_type] = attribution_table(\n",
    "        train_documents_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )\n",
    "\n",
    "    test_author_table_by_type[document_type] = attribution_table(\n",
    "        test_documents_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )\n",
    "\n",
    "    print(f'{document_type: <4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_by_type = {\n",
    "    'ISA' : Candidate(iterations=351, num_topics=40),\n",
    "    'PFR' : Candidate(iterations=251, num_topics=25),\n",
    "    'DPFR': Candidate(iterations=351, num_topics=25),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for document_type in election_by_type:\n",
    "    c = election_by_type[document_type]\n",
    "\n",
    "    corpus = tobows(\n",
    "        train_documents_by_type[document_type],\n",
    "        dictionary_by_type[document_type]\n",
    "    )[target]\n",
    "\n",
    "    atm = ATM(\n",
    "        corpus=list(corpus),\n",
    "        author2doc=train_author_table_by_type[document_type],\n",
    "        num_topics=c.num_topics,\n",
    "        iterations=c.iterations,\n",
    "    )\n",
    "    model_by_type[document_type] = atm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topics(model, doc_bow):\n",
    "\n",
    "    gamma_chunk, sstats = model.inference(\n",
    "        chunk=[doc_bow], author2doc=dict(), doc2author=dict(), \n",
    "        rhot=1.00,\n",
    "        collect_sstats=True\n",
    "    )\n",
    "\n",
    "    return gamma_chunk\n",
    "\n",
    "\n",
    "def get_model_author_topic_vectors(model):\n",
    "    author_topic_vectors = np.zeros(\n",
    "        (model.num_authors, model.num_topics)\n",
    "    )\n",
    "\n",
    "    for i, author in enumerate(model.id2author.values()):\n",
    "        idx, scores = zip(*model.get_author_topics(author))\n",
    "        author_topic_vectors[i, idx] = scores\n",
    "\n",
    "    return author_topic_vectors\n",
    "\n",
    "\n",
    "def get_sorted_authors(model, doc_bow, author_topic_vectors, metric=hellinger):\n",
    "    doc_vector = get_document_topics(model, doc_bow)\n",
    "\n",
    "    author_scores = np.argsort(\n",
    "        cdist(doc_vector, author_topic_vectors, metric=metric)\n",
    "    )\n",
    "\n",
    "    contenders = [\n",
    "        model.id2author[idx]\n",
    "        for idx in author_scores[0]\n",
    "    ]\n",
    "\n",
    "    return contenders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'prs_signatures.csv') as fd:\n",
    "    signatures = read_csv(fd)\n",
    "\n",
    "signatures.head()\n",
    "signatures.Users_ID = 'U' + signatures.Users_ID.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = signatures.drop_duplicates(['Users_ID'])[['Users_ID', 'First_Name', 'Middle_Name', 'Last_Name']].set_index('Users_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_type = 'ISA'\n",
    "\n",
    "model = model_by_type[document_type]\n",
    "author_topic_vectors = get_model_author_topic_vectors(model)\n",
    "vocabulary = dictionary_by_type[document_type]\n",
    "\n",
    "docs = [\n",
    "    'gimbal drive motor friction',\n",
    "    'rtg temperature charge capacity curiosity',\n",
    "    'rtg temperature charge capacity voyager',\n",
    "    'radiated image noise',\n",
    "    'ir cooler temperature',\n",
    "]\n",
    "\n",
    "for d in docs:\n",
    "    bow = vocabulary.doc2bow(d.split())\n",
    "\n",
    "    candidates = get_sorted_authors(model, bow, author_topic_vectors)\n",
    "\n",
    "    results = lookup.loc[Series(candidates)]\n",
    "    display(d)\n",
    "    display(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'prs.csv') as fd:\n",
    "    prs = read_csv(fd, low_memory=False)[\n",
    "        ['Anomaly_ID', 'Title', 'Description']\n",
    "    ]\n",
    "prs.Anomaly_ID = 'A' + prs.Anomaly_ID.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "U = 'U76104'\n",
    "P = authors_by_type[document_type][\n",
    "    authors_by_type[document_type].Users_ID == U\n",
    "]\n",
    "\n",
    "prs[(prs.Anomaly_ID.isin(P.Anomaly_ID)) &\n",
    "    #&((prs.Title.str.contains(\"(?i)\"+re.sub(\" \",\"|\",d),na=False))|(prs.Description.str.contains(\"(?i)\"+re.sub(\" \",\"|\",d),na=False)))\n",
    "    True\n",
    "   ][['Anomaly_ID', 'Title']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
