{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division#, unicode_literals\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pathlib2 import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "matplotlib.style.use('ggplot')\n",
    "%precision 4\n",
    "\n",
    "#NASA color palette\n",
    "nasa = {'red':'#fc3d21','blue':'#0b3d91','grey':'#79797c','black':'#000000'}\n",
    "\n",
    "BASEDIR = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PRS pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs = pd.read_csv(BASEDIR / 'prs.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format and subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeform text columns that potentially contain \"safing\"\n",
    "# cols_ff_text = ['title','description','correctiveAction','verificationAnalysis','issues','relatedDocuments',\n",
    "#                 'analysisImpacts','attachedFiles','testVerification','executiveSummary','procedure','rev',\n",
    "#                 'cogEClosurePlan','paragraph','rationale','cmfFileErrorDescription','cmfFileContributingCause',\n",
    "#                 'cmfFileProximateCause','cmfFileCorrectiveAction','cmfFileRootCause']\n",
    "\n",
    "cols_ff_text = ['Title','Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the text in free-form text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prs.columns = df_prs.columns.str.replace('Title', 'title').replace('Description', 'Description')\n",
    "df_prs.MainItemAffected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_words(row):\n",
    "    #Join all text in a report into a single string\n",
    "    words = ''\n",
    "    for col in cols_ff_text:\n",
    "        try:\n",
    "            words += ' ' + row[col]\n",
    "        except TypeError:\n",
    "            continue\n",
    "    return words\n",
    "\n",
    "'''\n",
    "    #Replace any punctuation, special characters, etc. with whitespace     \n",
    "    #words = re.sub('\\\\r|\\\\n|<br>|&quot;|[0-9]|\\.|,|:|;|\\(|\\)|\\[|\\]|{|}|<|>|\"|=|\\*+|- +',' ',words)\n",
    "    #Split words based on whitespace\n",
    "    words = words.split()\n",
    "    #Push words to lowercase\n",
    "    #words = [word.lower() for word in words]\n",
    "    #Set stemmer and use it to stem individual words\n",
    "    #st = LancasterStemmer()\n",
    "    #words = [st.stem(word) for word in words]\n",
    "    #Remove stopwords\n",
    "    #words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    #Return a string of cleaned words\n",
    "    return ' '.join(words)\n",
    "'''\n",
    "\n",
    "#Apply to main subset\n",
    "df_prs['words'] = df_prs.apply(get_words, axis=1)\n",
    "\n",
    "df_prs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe to model on which includes tiered structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tier structure data for MSL and M2020\n",
    "df_msl = pd.read_csv('../Data/PRS_MSL_Tier_Structure_160609.csv')\n",
    "print(df_msl.shape)\n",
    "\n",
    "df_m2020 = pd.read_csv('../Data/PRS_M2020_Tier_Structure_160609.csv')\n",
    "print(df_m2020.shape)\n",
    "\n",
    "#Create simplified 'item_number' feature\n",
    "df_msl['item_number'] = df_msl.apply(lambda row: re.sub('MSL[ -_]' , '', row['Item_Number']), axis=1)\n",
    "df_m2020['item_number'] = df_m2020.apply(lambda row: re.sub('M2020[ -_]' , '', row['Item_Number']), axis=1)\n",
    "\n",
    "#Create columns for tiers\n",
    "def create_tier_features(df,tier_max):\n",
    "    #Create empty tier columns    \n",
    "    for i in range(tier_max):\n",
    "        df['tier_{0}'.format(i)] = ''\n",
    "\n",
    "    #Fill in tier columns with values from \"Item_Acronym\"\n",
    "    for index,row in df.iterrows():\n",
    "        for i,tier in enumerate(re.split('-',row['Item_Acronym'])):\n",
    "            df.ix[index,'tier_{0}'.format(i)] = tier\n",
    "    \n",
    "create_tier_features(df_msl, 8)\n",
    "create_tier_features(df_m2020, 8)\n",
    "\n",
    "#Subset PRS for MSL data\n",
    "df_prs_msl = df_prs.ix[(df_prs['Project_Name']=='Mars Science Lab')&(df_prs['ReportType']=='PFR')].copy()\n",
    "\n",
    "#Values to drop from df_msl; insure that the join is 1-to-1\n",
    "#Option 1 - drop rows with duplicate \"Lifecycle_ID\" via \"item_number\"\n",
    "#Option 2 - guess, check with Leslie later\n",
    "drop_item_numbers = ['MSL 2000_FS','MSL 2009CABL']\n",
    "drop_item_acronym = ['FS-AVS-MCA-RMCA-BTE']\n",
    "drop_item_life_cycle_id = [16592]\n",
    "\n",
    "#Subset tier structure to remove values that produce duplicates when join is performed\n",
    "df_msl = df_msl[(~df_msl['Item_Number'].isin(drop_item_numbers))&\n",
    "                (~df_msl['Item_Acronym'].isin(drop_item_acronym))&\n",
    "                (~df_msl['Lifecycle_ID'].isin(drop_item_life_cycle_id))]\n",
    "\n",
    "#Perform join\n",
    "df_prs_msl = pd.merge(left=df_prs_msl,right=df_msl,how='left',left_on='MainItemAffected',right_on='Item_name')\n",
    "print(df_prs_msl.shape)\n",
    "\n",
    "#Subset PRS for M2020 data\n",
    "df_prs_m2020 = df_prs.ix[(df_prs['Project_Name']=='MARS 2020')&(df_prs['ReportType']=='PFR')].copy()\n",
    "\n",
    "#Perform join\n",
    "df_prs_m2020 = pd.merge(left=df_prs_m2020,right=df_m2020,how='left',left_on='MainItemAffected',right_on='Item_name')\n",
    "print(df_prs_m2020.shape)\n",
    "\n",
    "#Concat the dfs\n",
    "df_modeling = pd.concat([df_prs_msl,df_prs_m2020])\n",
    "\n",
    "#Reset index\n",
    "df_modeling = df_modeling.reset_index(drop=True)\n",
    "\n",
    "print(df_modeling.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_modeling = df_prs.ix[(df_prs['projectName'].isin(['Mars Science Lab','MARS 2020']))&\n",
    "                        (df_prs['reportType']=='PFR')].copy()\n",
    "df_modeling = df_modeling.reset_index()\n",
    "\n",
    "df_modeling.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine how many topics to select based on perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import lda as LatentDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get MSL ISAs\n",
    "df_modeling = df_prs[(df_prs['Project_Name']=='Mars Science Lab')&(df_prs['ReportType']=='ISA')&(~df_prs['Description'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add additional words to the list of English stop words\n",
    "additional_stop_words = ['test','tests','testing','tested','pfr','isa','quot','jpl','msl']\n",
    "additional_stop_words += ['08','09','10','11','2010','2011','2014']\n",
    "stop_words = ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "#Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=2000,\n",
    "                                stop_words=stop_words)\n",
    "tf = tf_vectorizer.fit_transform(df_modeling['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create base df\n",
    "df_perplexity = pd.DataFrame()\n",
    "\n",
    "#Specify parameters\n",
    "grid_lda = {5:[3] + list(np.arange(5,101,5)),\n",
    "            25:sorted([3, 5, 15] + list(np.arange(10,101,10))),\n",
    "            100: sorted([3, 5, 10, 15] + list(np.arange(20,101,20))),\n",
    "            250:sorted([3, 5, 15] + list(np.arange(10,101,10))),\n",
    "            500:sorted([3, 5, 15] + list(np.arange(10,101,10)))}\n",
    "\n",
    "for iters in grid_lda:\n",
    "    for topics in grid_lda[iters]:\n",
    "        print (\"Fitting {0} topics over {1} iterations...\".format(topics, iters))\n",
    "        lda = LatentDirichletAllocation(n_topics=topics, max_iter=iters,learning_method='online', learning_offset=50.,random_state=0)\n",
    "        lda.fit(tf)\n",
    "        temp_perplexity = lda.perplexity(tf)\n",
    "        df_perplexity = pd.concat([df_perplexity, pd.DataFrame([[iters, topics, temp_perplexity]], columns=[\"iters\",\"topics\",\"perplexity\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "ax = df_perplexity.ix[df_perplexity['iters']==5,['topics','perplexity']].plot(x='topics', figsize=(15,10), alpha=0.7)\n",
    "\n",
    "for i in [5,25,100,250,500]:\n",
    "    if i != 5:\n",
    "        df_perplexity.ix[df_perplexity['iters']==i,['topics','perplexity']].plot(x='topics', ax=ax, alpha=0.7)\n",
    "    df_temp_min = df_perplexity[df_perplexity['perplexity']==df_perplexity.ix[df_perplexity['iters']==i,'perplexity'].min()]\n",
    "    df_temp_min.plot(kind='scatter',x='topics',y='perplexity',ax=ax, s=150)\n",
    "    print(\"Min perplixity of {0:0.02f} at {1} topics with {2} iterations\".format(df_temp_min['perplexity'].values[0], df_temp_min['topics'].values[0], df_temp_min['iters'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save perplexity df\n",
    "# df_perplexity.to_csv('msl_description_perplexity.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit topic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define attributes for feature extraction (TF, TF-IDF) and LDA\n",
    "n_topics = 30\n",
    "max_iter = 500\n",
    "\n",
    "#USE SKLEARN LDA \n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=max_iter,learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(idx, topic.argsort()[:-5 - 1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print( \"Topic #{0}:\".format(topic_idx))\n",
    "        print( \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print( \"\")\n",
    "\n",
    "#Print topics\n",
    "print( \"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "#print_top_words(lda, tf_feature_names, n_top_words)\n",
    "print_top_words(lda, tf_feature_names, 50)\n",
    "print( lda.perplexity(tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Anomaly ID of interest\n",
    "anomaly_id = 57049\n",
    "#anomaly_id = 57069 #Carry over from - 13068\n",
    "anomaly_id = 57070 #Carry over from - 14612\n",
    "anomaly_id = 54892 #Carry over from - 15426\n",
    "\n",
    "#Get index\n",
    "pfr_index = df_modeling[df_modeling['anomalyID']==anomaly_id].index[0]\n",
    "\n",
    "#Tope N reports to keep\n",
    "top_n = 3\n",
    "\n",
    "#Distance columns\n",
    "dist_cols = ['euclidean_dist_to_{0}'.format(anomaly_id),\n",
    "             'cosine_dist_to_{0}'.format(anomaly_id),\n",
    "             'kullback_dist_to_{0}'.format(anomaly_id)]\n",
    "\n",
    "print( anomaly_id)\n",
    "print( pfr_index)\n",
    "print( df_modeling.ix[df_modeling['anomalyID']==anomaly_id,'Item_Acronym'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top n closest documents - no restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create distance df\n",
    "df_dist = pd.DataFrame()\n",
    "\n",
    "#Create base condition to build off of\n",
    "df_temp = df_modeling[df_modeling['projectName']=='Mars Science Lab']\n",
    "\n",
    "#Iterate through subsetted data\n",
    "for index,row in df_temp.iterrows():\n",
    "    #Calculate distances\n",
    "    euclidean_dist = scipy.spatial.distance.euclidean(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "    cosine_dist = scipy.spatial.distance.cosine(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "    kullback_dist = scipy.stats.entropy(lda.transform(tf[pfr_index]).T, lda.transform(tf[index]).T)[0]\n",
    "    #Create item_acronym based on location in traverse\n",
    "    temp_item_acronym = row['Item_Acronym']\n",
    "    cols = ['anomaly_id','tier','item_acronym'] + dist_cols\n",
    "    temp_df = pd.DataFrame([[row['anomalyID'], np.NaN, temp_item_acronym, euclidean_dist, cosine_dist, kullback_dist]], columns=cols)\n",
    "    df_dist = pd.concat([df_dist,temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset distance df \n",
    "df_dist_sub = pd.DataFrame()\n",
    "\n",
    "#Get top n \n",
    "for col in dist_cols:\n",
    "    df_dist_sub = pd.concat([df_dist_sub, df_dist.sort_values(col).head(top_n)])\n",
    "        \n",
    "#Create linkable url field\n",
    "df_dist_sub['url'] = df_dist_sub.apply(lambda row:\"https://prs.jpl.nasa.gov/NET/PFRReadOnly.aspx?smode=pop&iAnomalyID={0}\".format(row['anomaly_id']), axis=1)        \n",
    "\n",
    "for index,row in df_dist_sub.iterrows():\n",
    "    print \"{0}:\\t{1}\".format(row['anomaly_id'] ,row['url'])\n",
    "    \n",
    "df_dist_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top n closest documents - traverse tier structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create distance df\n",
    "df_dist_tier = pd.DataFrame()\n",
    "\n",
    "#Create base condition to build off of\n",
    "conditoinal_str = \"\"\n",
    "conditoinal_str = \"(df_modeling['projectName']=='Mars Science Lab')&\"\n",
    "\n",
    "#Get tier structure string based on provided Anomaly ID\n",
    "item_acronym = df_modeling.ix[df_modeling['anomalyID']==anomaly_id,\"Item_Acronym\"].values[0]\n",
    "item_acronym_list = re.split('-',item_acronym)\n",
    "\n",
    "#Traverse tier structure, increasing the specificity of conditions with each additional tier\n",
    "for i,tier in enumerate(item_acronym_list):\n",
    "    #Create new condition from current tier\n",
    "    temp_conditoinal_str = \"(df_modeling['tier_{0}']=='{1}')\".format(i, tier)\n",
    "    #If not the first tier add an \"&\"\n",
    "    if i > 0:\n",
    "        temp_conditoinal_str = \"&{0}\".format(temp_conditoinal_str)\n",
    "    conditoinal_str += temp_conditoinal_str\n",
    "    eval_str = \"df_modeling[{0}]\".format(conditoinal_str)\n",
    "    df_temp = eval(eval_str)\n",
    "\n",
    "    #Iterate through subsetted data\n",
    "    for index,row in df_temp.iterrows():\n",
    "        #Calculate distances\n",
    "        euclidean_dist = scipy.spatial.distance.euclidean(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "        cosine_dist = scipy.spatial.distance.cosine(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "        kullback_dist = scipy.stats.entropy(lda.transform(tf[pfr_index]).T, lda.transform(tf[index]).T)[0]\n",
    "        #Create item_acronym based on location in traverse\n",
    "        temp_item_acronym = \"-\".join(item_acronym_list[:i+1])\n",
    "        cols = ['anomaly_id',\n",
    "                'tier',\n",
    "                'item_acronym',\n",
    "                'euclidean_dist_to_{0}'.format(anomaly_id),\n",
    "                'cosine_dist_to_{0}'.format(anomaly_id),\n",
    "                'kullback_dist_to_{0}'.format(anomaly_id)]\n",
    "        temp_df = pd.DataFrame([[row['anomalyID'], i, temp_item_acronym, euclidean_dist, cosine_dist, kullback_dist]], columns=cols)\n",
    "        df_dist_tier = pd.concat([df_dist_tier,temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print anomaly_id\n",
    "\n",
    "#Subset distance df \n",
    "\n",
    "df_dist_tier_sub = pd.DataFrame()\n",
    "dist_cols = ['euclidean_dist_to_{0}'.format(anomaly_id),'cosine_dist_to_{0}'.format(anomaly_id),'kullback_dist_to_{0}'.format(anomaly_id)]\n",
    "for item in df_dist_tier['item_acronym'].unique():\n",
    "    df_temp = df_dist_tier[df_dist_tier['item_acronym']==item]\n",
    "    for col in dist_cols:\n",
    "        df_dist_tier_sub = pd.concat([df_dist_tier_sub, df_temp.sort_values(col).head(top_n)])\n",
    "        \n",
    "#df_dist_tier_sub['url'] = df_dist_tier_sub.apply(lambda row:\"https://prs.jpl.nasa.gov/NET/PFRReadOnly.aspx?smode=pop&iAnomalyID={0}\".format(row['anomaly_id']), axis=1)\n",
    "df_dist_tier_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment and when failure happened -- THIS DOESN'T WORK, LOOK AT 57070 AND THE ENV FAILURE RETURN VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "environment = df_modeling.ix[df_modeling['anomalyID']==anomaly_id,'specificEnvironment'].values[0]\n",
    "failed_during = df_modeling.ix[df_modeling['anomalyID']==anomaly_id,'problemFailureNotedDuring'].values[0]\n",
    "\n",
    "print \"Anomaly ID: {0}\".format(anomaly_id)\n",
    "print \"Environment: {0}\".format(environment)\n",
    "print \"Failure Noted During: {0}\".format(failed_during)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create distance df\n",
    "df_dist_env = pd.DataFrame()\n",
    "\n",
    "#Create base condition to build off of\n",
    "conditoinal_str = \"\"\n",
    "conditoinal_str = \"(df_modeling['projectName']=='Mars Science Lab')\"\n",
    "\n",
    "conditional_env_str = \"(df_modeling['specificEnvironment']=='{0}')\".format(environment)\n",
    "conditional_during_str = \"(df_modeling['problemFailureNotedDuring']=='{0}')\".format(failed_during)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Traverse tier structure, increasing the specificity of conditions with each additional tier\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        eval_str = \"df_modeling[{0}&{1}]\".format(conditoinal_str, conditional_env_str)\n",
    "        code = 'env'\n",
    "    elif i == 1:\n",
    "        eval_str = \"df_modeling[{0}&{1}]\".format(conditoinal_str, conditional_during_str)\n",
    "        code = 'during'\n",
    "    else:\n",
    "        eval_str = \"df_modeling[{0}&{1}&{2}]\".format(conditoinal_str, conditional_env_str, conditional_during_str)\n",
    "        code = 'env&during'\n",
    "    df_temp = eval(eval_str)\n",
    "\n",
    "    #Iterate through subsetted data\n",
    "    for index,row in df_temp.iterrows():\n",
    "        #Calculate distances\n",
    "        euclidean_dist = scipy.spatial.distance.euclidean(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "        cosine_dist = scipy.spatial.distance.cosine(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "        kullback_dist = scipy.stats.entropy(lda.transform(tf[pfr_index]).T, lda.transform(tf[index]).T)[0]\n",
    "        #Create item_acronym based on location in traverse\n",
    "        cols = ['anomaly_id',\n",
    "                'tier',\n",
    "                'item_acronym',\n",
    "                'euclidean_dist_to_{0}'.format(anomaly_id),\n",
    "                'cosine_dist_to_{0}'.format(anomaly_id),\n",
    "                'kullback_dist_to_{0}'.format(anomaly_id)]\n",
    "        temp_df = pd.DataFrame([[row['anomalyID'], i, code, euclidean_dist, cosine_dist, kullback_dist]], columns=cols)\n",
    "        df_dist_env = pd.concat([df_dist_env,temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print anomaly_id\n",
    "\n",
    "top_n = 1\n",
    "\n",
    "#Subset distance df \n",
    "df_dist_env_sub = pd.DataFrame()\n",
    "dist_cols = ['euclidean_dist_to_{0}'.format(anomaly_id),'cosine_dist_to_{0}'.format(anomaly_id),'kullback_dist_to_{0}'.format(anomaly_id)]\n",
    "\n",
    "for item in df_dist_env['item_acronym'].unique():\n",
    "    df_temp = df_dist_env[df_dist_env['item_acronym']==item]\n",
    "    for col in dist_cols:\n",
    "        df_dist_env_sub = pd.concat([df_dist_env_sub, df_temp.sort_values(col).head(top_n)])\n",
    "        \n",
    "df_dist_env_sub['url'] = df_dist_env_sub.apply(lambda row:\"https://prs.jpl.nasa.gov/NET/PFRReadOnly.aspx?smode=pop&iAnomalyID={0}\".format(row['anomaly_id']), axis=1)\n",
    "\n",
    "for index,row in df_dist_env_sub.iterrows():\n",
    "    print \"{0}:\\t{1}\\t{2}\".format(row['anomaly_id'] , row['item_acronym'], row['url'])\n",
    "\n",
    "df_dist_env_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.ix[(df_modeling['projectName']=='MARS 2020')&(~df_modeling['failureEffectRatingValue'].isnull()),\n",
    "               ['anomalyID','failureEffectRatingValue','projectName','title']].sort_values('failureEffectRatingValue',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col + anomaly_id for col in ['euclidean_dist_to_','cosine_dist_to_','kullback_dist_to_']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly ID to compare against\n",
    "anomaly_id = 57069\n",
    "#anomaly_id = 58481\n",
    "\n",
    "#Get the index for the \n",
    "pfr_index = df_modeling.ix[df_modeling['anomalyID']==anomaly_id].index[0]\n",
    "\n",
    "df_dist = pd.DataFrame()\n",
    "\n",
    "for index,row in df_modeling.iterrows():\n",
    "    euclidean_dist = scipy.spatial.distance.euclidean(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "    cosine_dist = scipy.spatial.distance.cosine(lda.transform(tf[pfr_index]), lda.transform(tf[index]))\n",
    "    kullback_dist = scipy.stats.entropy(lda.transform(tf[pfr_index]).T, lda.transform(tf[index]).T)[0]\n",
    "    cols = ['anomaly_id',\n",
    "            'euclidean_dist_to_{0}'.format(anomaly_id),\n",
    "            'cosine_dist_to_{0}'.format(anomaly_id),\n",
    "            'kullback_dist_to_{0}'.format(anomaly_id)]\n",
    "    temp_df = pd.DataFrame([[row['anomalyID'], euclidean_dist, cosine_dist, kullback_dist]], columns=cols)\n",
    "    df_dist = pd.concat([df_dist,temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist.sort_values('euclidean_dist_to_{0}'.format(anomaly_id)).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist.sort_values('cosine_dist_to_{0}'.format(anomaly_id)).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist.sort_values('kullback_dist_to_{0}'.format(anomaly_id)).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "#print \"\\nTopics in LDA model:\"\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "#print_top_words(lda, tf_feature_names, n_top_words)\n",
    "#print lda.perplexity(tf)\n",
    "d1 = tf[0]\n",
    "print [tf_feature_names[i] for i in d1.indices]\n",
    "df_modeling.ix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tier structure data for MSL and M2020\n",
    "\n",
    "df_msl = pd.read_csv('../Data/PRS_MSL_Tier_Structure_160609.csv')\n",
    "print df_msl.shape\n",
    "\n",
    "df_m2020 = pd.read_csv('../Data/PRS_M2020_Tier_Structure_160609.csv')\n",
    "print df_m2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create simplified 'item_number' feature\n",
    "df_msl['item_number'] = df_msl.apply(lambda row: re.sub('MSL[ -_]' , '', row['Item_Number']), axis=1)\n",
    "df_m2020['item_number'] = df_m2020.apply(lambda row: re.sub('M2020[ -_]' , '', row['Item_Number']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that provides high-level descriptive counts \n",
    "def project_tier_structure_description(df, project_name, report_type):\n",
    "\n",
    "    print \"---- {0} ----\".format(project_name)\n",
    "    print \"Unique items in tier structure: {0}\".format(df['Item_name'].nunique())\n",
    "    print \"Unique items in {0}: {1}\".format(report_type, df_prs.ix[df_prs['projectName']==project_name,'mainItemAffected'].nunique())\n",
    "\n",
    "    #Unique items from msl prs\n",
    "    items_prs = df_prs.ix[df_prs['projectName']==project_name,'mainItemAffected'].unique()\n",
    "    items_prs_set = set(items_prs[~pd.isnull(items_prs)])\n",
    "    items_prs_set = set([str(i) for i in items_prs_set])\n",
    "\n",
    "    #Unique items from msl tier structure\n",
    "    items_tier_struct = df['Item_name'].unique()\n",
    "    items_ts_set = set(items_tier_struct[~pd.isnull(items_tier_struct)])\n",
    "\n",
    "    #Check for superset/subset\n",
    "    print \"\\nAll items in PRS project contained in tier structure: {0}\".format(items_ts_set.issuperset(items_prs_set))\n",
    "    #print items_prs_set.issubset(items_ts_set)\n",
    "\n",
    "    #Get the items not in tier structure and the unique count\n",
    "    if not items_ts_set.issuperset(items_prs_set):\n",
    "        x = sorted(items_prs_set.difference(items_ts_set))\n",
    "    print \"Number of unique items not in tier structure: {0}\".format(len(x))\n",
    "\n",
    "    #Total number of reports for a project\n",
    "    total_reports = df_prs.ix[(df_prs['projectName']==project_name)&(df_prs['reportType']==report_type)].shape[0]\n",
    "    \n",
    "    #Total number of reports for a project that reference a part in the tier structure\n",
    "    total_reports_with_ts_item = df_prs.ix[(df_prs['projectName']==project_name)&\n",
    "                                           (df_prs['reportType']==report_type)&\n",
    "                                           (df_prs['mainItemAffected'].isin(items_ts_set))].shape[0]\n",
    "\n",
    "    print \"\\nTotal {0} {1}s: {2}\".format(project_name, report_type, total_reports)\n",
    "    print \"Total {0} {1}s with item in Tier Structure: {2}\".format(project_name, report_type, total_reports_with_ts_item)\n",
    "    print \"Total {0} {1}s with item NOT in Tier Structure: {2}\".format(project_name, report_type, total_reports - total_reports_with_ts_item)\n",
    "    print \n",
    "    \n",
    "    return x\n",
    "    \n",
    "x = project_tier_structure_description(df_msl, \"Mars Science Lab\", \"PFR\")\n",
    "#project_tier_structure_description(df_msl, \"Mars Science Lab\", \"DPFR\")\n",
    "#project_tier_structure_description(df_msl, \"Mars Science Lab\", \"ISA\")\n",
    "\n",
    "x2 = project_tier_structure_description(df_msl, \"MARS 2020\", \"PFR\")\n",
    "#project_tier_structure_description(df_msl, \"MARS 2020\", \"DPFR\")\n",
    "#project_tier_structure_description(df_msl, \"MARS 2020\", \"ISA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs.ix[(df_prs['projectName']==\"Mars Science Lab\")&\n",
    "                                           (df_prs['reportType']==\"PFR\")&\n",
    "                                           (df_prs['mainItemAffected']==\"3/4 Pyro Valve\"), 'anomalyID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item_Acronym - Tier Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print df_msl.shape\n",
    "print df_m2020.shape\n",
    "print\n",
    "\n",
    "print df_msl['Item_Acronym'].nunique()\n",
    "print df_m2020['Item_Acronym'].nunique()\n",
    "print\n",
    "\n",
    "print df_msl[df_msl['Item_Acronym'].isin(df_m2020['Item_Acronym'].unique())].shape\n",
    "print df_m2020[df_m2020['Item_Acronym'].isin(df_msl['Item_Acronym'].unique())].shape\n",
    "\n",
    "df_m2020[~df_m2020['Item_Acronym'].isin(df_msl['Item_Acronym'].unique())].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2020[df_m2020['Item_Acronym'].str.contains(\"- .*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2020[df_m2020['Item_Acronym'].str.contains(\"BRLA\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create columns for tiers\n",
    "def create_tier_features(df,tier_max):\n",
    "    #Create empty tier columns    \n",
    "    for i in range(tier_max):\n",
    "        df['tier_{0}'.format(i)] = ''\n",
    "\n",
    "    #Fill in tier columns with values from \"Item_Acronym\"\n",
    "    for index,row in df.iterrows():\n",
    "        for i,tier in enumerate(re.split('-',row['Item_Acronym'])):\n",
    "            df.ix[index,'tier_{0}'.format(i)] = tier\n",
    "    \n",
    "create_tier_features(df_msl, 8)\n",
    "create_tier_features(df_m2020, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset PRS for MSL data\n",
    "df_prs_msl = df_prs.ix[(df_prs['projectName']=='Mars Science Lab')&(df_prs['reportType']=='PFR')].copy()\n",
    "\n",
    "#Values to drop from df_msl; insure that the join is 1-to-1\n",
    "#Option 1 - drop rows with duplicate \"Lifecycle_ID\" via \"item_number\"\n",
    "#Option 2 - guess, check with Leslie later\n",
    "drop_item_numbers = ['MSL 2000_FS','MSL 2009CABL']\n",
    "drop_item_acronym = ['FS-AVS-MCA-RMCA-BTE']\n",
    "drop_item_life_cycle_id = [16592]\n",
    "\n",
    "#Subset tier structure to remove values that produce duplicates when join is performed\n",
    "df_msl = df_msl[(~df_msl['Item_Number'].isin(drop_item_numbers))&\n",
    "                (~df_msl['Item_Acronym'].isin(drop_item_acronym))&\n",
    "                (~df_msl['Lifecycle_ID'].isin(drop_item_life_cycle_id))]\n",
    "\n",
    "#Perform join\n",
    "df_prs_msl = pd.merge(left=df_prs_msl,right=df_msl,how='left',left_on='mainItemAffected',right_on='Item_name')\n",
    "\n",
    "print df_prs_msl.shape\n",
    "print df_temp.shape\n",
    "\n",
    "#Subset PRS for MSL data\n",
    "df_prs_m2020 = df_prs.ix[(df_prs['projectName']=='MARS 2020')&(df_prs['reportType']=='PFR')].copy()\n",
    "\n",
    "#Perform join\n",
    "df_prs_m2020 = pd.merge(left=df_prs_m2020,right=df_m2020,how='left',left_on='mainItemAffected',right_on='Item_name')\n",
    "\n",
    "print df_prs_m2020.shape\n",
    "print df_prs_m2020.shape\n",
    "\n",
    "#Concat the dfs\n",
    "df_prs_msl_m2020 = pd.concat([df_prs_msl,df_prs_m2020])\n",
    "df_prs_msl_m2020 = df_prs_msl_m2020.reset_index(drop=True)\n",
    "\n",
    "print df_prs_msl_m2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traverse tier structure to give more relevant results\n",
    "\n",
    "#Set anomaly ID of interest\n",
    "anomaly_id = df_prs_msl_m2020.ix[4671,\"anomalyID\"]\n",
    "\n",
    "#Traverse tier structure, increasing the specificity of conditions with each additional tier\n",
    "item_acronym = df_prs_msl_m2020.ix[df_prs_msl_m2020['anomalyID']==anomaly_id,\"Item_Acronym\"].values[0]\n",
    "conditoinal_str = \"(df_prs_msl_m2020['projectName']=='Mars Science Lab')&\"\n",
    "for i,tier in enumerate(re.split('-',item_acronym)):\n",
    "    print i,tier\n",
    "    temp_conditoinal_str = \"(df_prs_msl_m2020['tier_{0}']=='{1}')\".format(i, tier)\n",
    "    if i > 0:\n",
    "        temp_conditoinal_str = \"&{0}\".format(temp_conditoinal_str)\n",
    "    conditoinal_str += temp_conditoinal_str\n",
    "    eval_str = \"df_prs_msl_m2020[{0}].shape\".format(conditoinal_str)\n",
    "    print eval_str\n",
    "    print eval(eval_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "df_prs_msl_m2020[(df_prs_msl_m2020['projectName']=='Mars Science Lab')&\n",
    "                 (df_prs_msl_m2020['tier_1']=='ICheMin')]\n",
    "df_prs_msl_m2020[df_prs_msl_m2020['tier_1']=='ICheMin']\n",
    "df_prs_msl_m2020['tier_1'].unique()\n",
    "df_prs_msl_m2020.ix[df_prs_msl_m2020['anomalyID']==13274,'mainItemAffected']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Option 1 - drop rows with duplicate \"Lifecycle_ID\" via \"item_number\"\n",
    "#Option 2 - guess, check with Leslie later\n",
    "\n",
    "print df_msl.shape\n",
    "print df_msl['Item_name'].nunique()\n",
    "\n",
    "gb_msl = df_msl.groupby('Item_name').count()\n",
    "x  = df_msl[df_msl['Item_name'].isin(gb_msl[gb_msl['Lifecycle_Name']>1].index)]\n",
    "\n",
    "drop_item_numbers = ['MSL 2000_FS','MSL 2009CABL']\n",
    "drop_item_acronym = ['FS-AVS-MCA-RMCA-BTE']\n",
    "drop_item_life_cycle_id = [16592]\n",
    "\n",
    "x[(~x['Item_Number'].isin(drop_item_numbers))&(~x['Item_Acronym'].isin(drop_item_acronym))&(~x['Lifecycle_ID'].isin(drop_item_life_cycle_id))]\n",
    "\n",
    "sorted(df_msl['Item_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp[(df_temp['tier_0']=='FS')&(df_temp['tier_1']=='AVS')&(df_temp['tier_2']=='MCA')&(df_temp['tier_3'].isin(['DMCA','RMCA']))].groupby('tier_3').count()\n",
    "\n",
    "df_temp[df_temp['Item_Acronym'].isin(['FS-AVS-MCA-DMCA-BTE','FS-AVS-MCA-RMCA-BTE'])].groupby('tier_3').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs_msl[df_prs_msl['mainItemAffected'].isin(gb_msl[gb_msl['Lifecycle_Name']>1].index)].groupby('mainItemAffected').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs_msl[df_prs_msl['mainItemAffected']=='MCA Bench Test Equipment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item_name - part?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2020[df_m2020['Item_name'].str.contains(\"Actuators\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msl[df_msl['Item_Acronym'].str.contains(\"FS-\")]\n",
    "df_msl[df_msl['Item_name'].str.contains(\"Actuators\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_msl.shape\n",
    "print df_msl[df_msl['Item_Number'].str.contains(\"MSL[ -_]\")].shape\n",
    "#print df_msl.ix[~df_msl['Item_Number'].str.contains(\"MSL \"),'Item_Number']\n",
    "\n",
    "print df_m2020.shape\n",
    "print df_m2020[df_m2020['Item_Number'].str.contains(\"M2020[ -_]\")].shape\n",
    "print df_m2020.ix[~df_m2020['Item_Number'].str.contains(\"M2020[ -_]\"),'Item_Number']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_m2020['item_number'].unique()).difference(set(df_msl['item_number'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs[df_prs['projectName']=='Mars 2020']  #.groupby('reportType').count()['_id']\n",
    "sorted(df_prs['projectName'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_id = 49580\n",
    "df_modeling.ix[df_modeling['anomalyID']==anomaly_id,'words'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset data to provide labels for\n",
    "df_prs_topics = df_prs[df_prs['projectName'].isin(df_part_regex['project_name'].unique())].copy()\n",
    "print df_prs_topics.shape\n",
    "print df_prs_topics[df_prs_topics['_id'].isin(df_modeling['_id'])].shape\n",
    "\n",
    "#Subset data to provide labels for\n",
    "tf_new = tf_vectorizer.transform(df_prs_topics['words'])\n",
    "\n",
    "#Get topics for subset\n",
    "topics_lda_new = lda.transform(tf_new)\n",
    "topics_lda_new /= topics_lda_new.sum(axis=1).reshape((tf_new.shape[0],1))\n",
    "\n",
    "#Create a new column for each topic\n",
    "for i in range(n_topics):\n",
    "    df_prs_topics[\"topic_{0}\".format(i)] = topics_lda_new[:,i]\n",
    "    \n",
    "df_prs_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_prs['causeCodes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
