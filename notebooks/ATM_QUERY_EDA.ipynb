{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, Counter\n",
    "from glob import glob\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from os import remove\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel as LDA\n",
    "from gensim.models import AuthorTopicModel as ATM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "\n",
    "import pyLDAvis as ldavis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import coo_matrix as sparse_matrix\n",
    "\n",
    "ldavis.enable_notebook()\n",
    "%matplotlib notebook\n",
    "#%precision 4\n",
    "\n",
    "\n",
    "Candidate = namedtuple('Candidate', ['iterations', 'num_topics'])\n",
    "\n",
    "\n",
    "\n",
    "def get_i_t(filename):\n",
    "    _, content, document_type = filename.split('-')\n",
    "    i, t, _ = content.split('_')\n",
    "    return int(i[1:]), int(t[1:]), document_type.split('.')[0]\n",
    "\n",
    "get_texts = lambda df: df[target].str.split()\n",
    "tobows = lambda df, d: concat([df['Anomaly_ID'], get_texts(df).apply(d.doc2bow)], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "isr2 = 2.0 ** -.5\n",
    "\n",
    "def hellinger(x, y):\n",
    "    return isr2 * np.sqrt(((np.sqrt(x) - np.sqrt(y)) ** 2).sum())\n",
    "\n",
    "\n",
    "\n",
    "report_types = 'ISA', 'PFR', 'DPFR'\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "min_occurances = 2\n",
    "target = 'GLOMUNSTEM'\n",
    "\n",
    "%matplotlib notebook\n",
    "%precision 4\n",
    "\n",
    "BASEDIR = Path('../data')\n",
    "OUT = Path('../output/atm/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'processed_authors.csv') as fd:\n",
    "    af = read_csv(fd)\n",
    "af.shape\n",
    "af.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_type = {\n",
    "    t: af[af.ReportType == t] \n",
    "    for t in af.ReportType.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type = {\n",
    "    t: read_csv(OUT / f'../norm_{t}.csv').dropna()\n",
    "    for t in af.ReportType.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_document_downselect(documents, authors, ANOMALY_LABEL='Anomaly_ID'):\n",
    "    documents[ANOMALY_LABEL] = \\\n",
    "      'A' + documents[ANOMALY_LABEL].apply(str)\n",
    "\n",
    "    idx = documents[ANOMALY_LABEL].isin(\n",
    "        authors[ANOMALY_LABEL].unique()\n",
    "    )\n",
    "\n",
    "    documents = documents[idx]\n",
    "    \n",
    "    idx = authors[ANOMALY_LABEL].isin(\n",
    "        documents[ANOMALY_LABEL].unique()\n",
    "    )\n",
    "    authors = authors[idx]\n",
    "    return documents, authors\n",
    "\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    documents_by_type[document_type], authors_by_type[document_type] = \\\n",
    "        author_document_downselect(\n",
    "          documents_by_type[document_type],\n",
    "          authors_by_type[document_type]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type['ISA'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_and_vocab(documents, test_size, min_occurances=min_occurances):\n",
    "    train, test = train_test_split(\n",
    "        documents, test_size=test_size\n",
    "    )\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    vocab = Dictionary(\n",
    "        train[target].str.split()\n",
    "    )\n",
    "    vocab.filter_extremes(no_below=min_occurances)\n",
    "\n",
    "    return train, test, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_by_type, train_documents_by_type, test_documents_by_type, dictionary_by_type = \\\n",
    "  dict(), dict(), dict(), dict()\n",
    "\n",
    "train_author_table_by_type = dict()\n",
    "test_author_table_by_type = dict()\n",
    "\n",
    "def attribution_table(documents, relevent_authors):\n",
    "    store = defaultdict(set)\n",
    "    for idx, anomaly in documents.iterrows():\n",
    "        authors_documents = relevent_authors[\n",
    "            relevent_authors.Anomaly_ID == anomaly.Anomaly_ID\n",
    "        ]\n",
    "\n",
    "        for author in authors_documents.Users_ID:\n",
    "            store[author].add(idx)\n",
    "\n",
    "    return {k: list(v) for k, v in store.items()}\n",
    "\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    train_documents_by_type[document_type], \\\n",
    "    test_documents_by_type[document_type], \\\n",
    "    dictionary_by_type[document_type] = \\\n",
    "        get_train_test_and_vocab(documents_by_type[document_type], TEST_SIZE)\n",
    "\n",
    "    train_author_table_by_type[document_type] = attribution_table(\n",
    "        train_documents_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )\n",
    "\n",
    "    test_author_table_by_type[document_type] = attribution_table(\n",
    "        test_documents_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )\n",
    "\n",
    "    print(f'{document_type: <4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_by_type = {\n",
    "    'ISA' : Candidate(iterations=351, num_topics=40),\n",
    "    'PFR' : Candidate(iterations=251, num_topics=25),\n",
    "    'DPFR': Candidate(iterations=351, num_topics=25),\n",
    "}\n",
    "\n",
    "for document_type in election_by_type:\n",
    "    c = election_by_type[document_type]\n",
    "\n",
    "    corpus = tobows(\n",
    "        train_documents_by_type[document_type],\n",
    "        dictionary_by_type[document_type]\n",
    "    )[target]\n",
    "\n",
    "    atm = ATM(\n",
    "        corpus=list(corpus),\n",
    "        author2doc=train_author_table_by_type[document_type],\n",
    "        num_topics=c.num_topics,\n",
    "        iterations=c.iterations,\n",
    "    )\n",
    "    model_by_type[document_type] = atm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topics(model, doc_bow):\n",
    "\n",
    "    gamma_chunk, sstats = model.inference(\n",
    "        chunk=[doc_bow], author2doc=dict(), doc2author=dict(), \n",
    "        rhot=1.00,\n",
    "        collect_sstats=True\n",
    "    )\n",
    "\n",
    "    return gamma_chunk\n",
    "\n",
    "\n",
    "def get_model_author_topic_vectors(model):\n",
    "    author_topic_vectors = np.zeros(\n",
    "        (model.num_authors, model.num_topics)\n",
    "    )\n",
    "\n",
    "    for i, author in enumerate(model.id2author.values()):\n",
    "        idx, scores = zip(*model.get_author_topics(author))\n",
    "        author_topic_vectors[i, idx] = scores\n",
    "\n",
    "    return author_topic_vectors\n",
    "\n",
    "\n",
    "def get_sorted_authors(model, doc_bow, author_topic_vectors, metric=hellinger):\n",
    "    doc_vector = get_document_topics(model, doc_bow)\n",
    "\n",
    "    author_scores = np.argsort(\n",
    "        cdist(doc_vector, author_topic_vectors, metric=metric)\n",
    "    )\n",
    "\n",
    "    contenders = [\n",
    "        model.id2author[idx]\n",
    "        for idx in author_scores[0]\n",
    "    ]\n",
    "\n",
    "    return contenders\n",
    "\n",
    "\n",
    "def get_all_ranks_and_counts(model, publication_counts, authors, dictionary, documents):\n",
    "    author_topic_vectors = get_model_author_topic_vectors(model)\n",
    "\n",
    "    for idx, row in tobows(documents, dictionary).iterrows():\n",
    "        Anomaly_ID, doc_bow = row['Anomaly_ID'], row[target]\n",
    "\n",
    "        contenders = get_sorted_authors(model, doc_bow, author_topic_vectors)\n",
    "        \n",
    "        real_authors = [\n",
    "            a\n",
    "            for a in authors\n",
    "            if idx in authors[a]\n",
    "        ]\n",
    "\n",
    "        for a in real_authors:\n",
    "            try:\n",
    "                rank = contenders.index(a)\n",
    "            except ValueError as e:\n",
    "                rank = 500   \n",
    "\n",
    "            try:\n",
    "                publications = publication_counts[a]\n",
    "            except KeyError:\n",
    "                publications = 0\n",
    "\n",
    "            yield publications, rank\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_author_table_by_type\n",
    "\n",
    "TT = 'ISA'\n",
    "\n",
    "publication_counts_by_type = dict()\n",
    "for t in train_author_table_by_type:\n",
    "    _authors = train_author_table_by_type[t]\n",
    "    publication_counts_by_type[t] = {a: len(_authors[a]) for a in _authors}\n",
    "    del _authors\n",
    "\n",
    "train = list(get_all_ranks_and_counts(\n",
    "    model_by_type[TT],\n",
    "    publication_counts_by_type[TT],\n",
    "    train_author_table_by_type[TT],\n",
    "    dictionary_by_type[TT],\n",
    "    train_documents_by_type[TT]\n",
    "))\n",
    "\n",
    "test = list(get_all_ranks_and_counts(\n",
    "    model_by_type[TT],\n",
    "    publication_counts_by_type[TT],\n",
    "    test_author_table_by_type[TT],\n",
    "    dictionary_by_type[TT],\n",
    "    test_documents_by_type[TT]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_documents_by_type[TT].shape)\n",
    "print(train_documents_by_type[TT].iloc[0][target])\n",
    "\n",
    "from itertools import chain\n",
    "#set(chain(*train_author_table_by_type['ISA'].values()))\n",
    "#set(chain(*test_author_table_by_type['ISA'].values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arc(arc):\n",
    "    _ = plt.figure()\n",
    "    # Load the dataset\n",
    "    data = DataFrame(columns=[\"publications\", \"rank\"], data=arc)\n",
    "    _ = sns.jointplot(x=data['publications'], y=data['rank'], \n",
    "                      kind='hex', \n",
    "                      xlim=(0, 250), ylim=(0, 300), gridsize=30\n",
    "                     )\n",
    "\n",
    "_ = plot_arc(train)\n",
    "\n",
    "_ = plot_arc(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(arc, label, top):\n",
    "    _ = plt.figure()\n",
    "    # Load the dataset\n",
    "    ranks = np.array(arc, dtype=np.float)[:,1]\n",
    "    #print(ranks)\n",
    "    \n",
    "    def get_stats(ranks):\n",
    "        for cap in range(500):\n",
    "            # there are no negatives\n",
    "            idx = ranks < cap\n",
    "            accuracy = np.sum(idx) / len(idx) * 100\n",
    "\n",
    "            yield cap, accuracy\n",
    "    x, y = zip(*list(get_stats(ranks)))\n",
    "    plt.scatter(x[1:], [k-j for j, k in zip(y, y[1:])], alpha=.1)\n",
    "    plt.axvline(x=top, color='r')\n",
    "    plt.title(f'{label} {y[top]:.2f}% at K={top}')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('improved recall at K')\n",
    "\n",
    "    \n",
    "top = 15\n",
    "plot(train, f'{TT}-train', top)\n",
    "plot(test, f'{TT}-test', top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 2\n",
    "keys = np.array(test, dtype=np.float)[:,1] <= top\n",
    "keys.sum()\n",
    "QUERY_DOCS = test_documents_by_type[TT][keys]\n",
    "L = QUERY_DOCS.GLOMUNSTEM.apply(lambda x: len(str.split(x)))\n",
    "L.median(), L.mean(), L.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_DOCS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def subsample_bow(bow, size, iterations):\n",
    "    doc = []\n",
    "    for key, count in bow:\n",
    "        doc.extend([key for _ in range(count)])\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        if size >= len(doc):\n",
    "            continue\n",
    "        yield list(Counter(sample(doc, size)).items())\n",
    "\n",
    "\n",
    "def query_strength_investigation(\n",
    "    model, authors, dictionary, documents,\n",
    "    iterations, sample_sizes\n",
    "):\n",
    "    author_topic_vectors = get_model_author_topic_vectors(model)\n",
    "\n",
    "    for idx, row in tobows(documents, dictionary).iterrows():\n",
    "        Anomaly_ID, doc_bow = row['Anomaly_ID'], row[target]\n",
    "\n",
    "        for size in sample_sizes:\n",
    "            for new_bow in subsample_bow(doc_bow, size, iterations):\n",
    "\n",
    "                contenders = get_sorted_authors(\n",
    "                    model, new_bow, author_topic_vectors\n",
    "                )\n",
    "\n",
    "                real_authors = [\n",
    "                    a\n",
    "                    for a in authors\n",
    "                    if idx in authors[a]\n",
    "                ]\n",
    "\n",
    "                for a in real_authors:\n",
    "                    try:\n",
    "                        rank = contenders.index(a)\n",
    "                    except ValueError as e:\n",
    "                        rank = 500\n",
    "\n",
    "                    yield size, rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = list(query_strength_investigation(\n",
    "    model_by_type[TT],\n",
    "    test_author_table_by_type[TT],\n",
    "    dictionary_by_type[TT],\n",
    "    QUERY_DOCS, 100, list(range(4, 100, 5))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = zip(*sorted(Q, key=itemgetter(0)))\n",
    "\n",
    "_ = plt.figure()\n",
    "_ = sns.boxplot(\n",
    "    x=x, y=y, \n",
    "    palette=\"Set3\")\n",
    "plt.xticks(rotation=70)\n",
    "plt.ylim((0, 25))\n",
    "plt.axhline(y=5, color='r')\n",
    "plt.axhline(y=10, color='r')\n",
    "plt.axhline(y=15, color='r')\n",
    "plt.title('Good Test Document Subset Investigation')\n",
    "plt.xlabel('Random subset # of critical Words')\n",
    "_ = plt.ylabel('New Ranking from document with R <=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY_DOCS.GLOMUNSTEM.apply(lambda s: len(s.split())).argsort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elm = 10\n",
    "QUERY_DOCS.GLOMUNSTEM.loc[elm]\n",
    "#QUERY_DOCS.GLOMPRE.loc[elm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = \\\n",
    "    (np.array(test, dtype=np.float)[:,1] >=10) & \\\n",
    "    (np.array(test, dtype=np.float)[:,1] <= 20)\n",
    "\n",
    "keys.sum()\n",
    "MID_QUERY_DOCS = test_documents_by_type[TT][keys]\n",
    "L = MID_QUERY_DOCS.GLOMUNSTEM.apply(lambda x: len(str.split(x)))\n",
    "L.median(), L.mean(), L.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MID_QUERY_DOCS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = list(query_strength_investigation(\n",
    "    model_by_type[TT],\n",
    "    test_author_table_by_type[TT],\n",
    "    dictionary_by_type[TT],\n",
    "    MID_QUERY_DOCS, 100, list(range(4, 100, 5))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*sorted(R, key=itemgetter(0)))\n",
    "\n",
    "_ = plt.figure()\n",
    "_ = sns.boxplot(\n",
    "    x=x, y=y,\n",
    "    palette=\"Set3\")\n",
    "plt.ylim((0, 80))\n",
    "plt.xticks(rotation=70)\n",
    "plt.axhline(y=20, color='r')\n",
    "#plt.axhline(y=10, color='r')\n",
    "#plt.axhline(y=15, color='r')\n",
    "plt.title('Mediocore Test Document Subset Investigation')\n",
    "plt.xlabel('Random subset # of critical Words')\n",
    "_ = plt.ylabel('New Ranking from document with (10 <= R <=20)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keys = \\\n",
    "    (np.array(test, dtype=np.float)[:,1] > 100) & \\\n",
    "    (np.array(test, dtype=np.float)[:,1] < 500)\n",
    "\n",
    "\n",
    "BAD_QUERY_DOCS = test_documents_by_type[TT][keys]\n",
    "display(BAD_QUERY_DOCS.GLOMPRE.iloc[0])\n",
    "display(BAD_QUERY_DOCS[target].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
