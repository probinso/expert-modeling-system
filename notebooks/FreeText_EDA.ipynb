{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "from math import isnan\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "import seaborn as sns\n",
    "\n",
    "from lang import english, clean_stems\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "%precision 4\n",
    "\n",
    "BASEDIR = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'prs.csv') as fd:\n",
    "    df = read_csv(fd, low_memory=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_columns = ['AssigneeUserID', 'OriginatorUserID', 'ResponsibleEditorUserId', 'SignersUserID']\n",
    "censored_authors = [43660., 75254., 75426., 76744., 78939., 80050., 82903., 83760.]\n",
    "idx = df.applymap(lambda i: i in censored_authors).any(axis=1)\n",
    "display(df[idx])\n",
    "df = df[~idx]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_anomalies = [\n",
    "    55793, 39583, 39582, 22103, 8789, 5011, #PFR\n",
    "    54721, 36880, 16113, 13705, 13622, 5640, 4521, 4371, 4045, 3151, 2385, 2207, #DPFR\n",
    "    41963, 37011, 28842, 23095, 22818, 17127, 14871, 13982 #ISA\n",
    "]\n",
    "idx = df.Anomaly_ID.isin(censored_anomalies)\n",
    "df = df[~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_num(s):\n",
    "    s_sp = s.split(\" :: \", 1)\n",
    "    if len(s_sp) > 1:\n",
    "        return s_sp\n",
    "    else:\n",
    "        return '', s_sp[0]\n",
    "\n",
    "\n",
    "def rm_dup_pfr(pfr):\n",
    "    # remove dummy test data from PRS training/dev\n",
    "    pfr = pfr[pfr['Project_Name'] != 'PRS Training Project']\n",
    "    pfr = pfr.drop([col for col in pfr.columns \\\n",
    "                    if 'PRS Training Project' in col], axis=1)\n",
    "\n",
    "    # separate report numbers from titles that are migrated from legacy system\n",
    "    pfr['rep_num'], pfr['rep_title'] = pfr['Title'].apply(lambda x: get_rep_num(x)).str\n",
    "\n",
    "    # remove one long report number which is duplicate of an ISA report\n",
    "    pfr = pfr[pfr.rep_num.str.len() < 7]\n",
    "\n",
    "    # remove duplicates in report description \n",
    "    # Reports may be generated multiple times due to system glitches in PRS\n",
    "    pfr = pfr[-pfr.Description.str.lower().duplicated()]\n",
    "\n",
    "    # use keywords in report titles to remove shadow/companion D/PFR for ISA\n",
    "    title_dup_key = ['duplicate ', 'shadow d?pfr', 'companion d?pfr']\n",
    "    pfr = pfr[-pfr.rep_title.str.lower().str.contains('|'.join(title_dup_key))]\n",
    "\n",
    "    # use keywords in report descriptions to remove duplicates\n",
    "    desc_dup_key = ['coordinating pfr', 'duplicate of', 'administrative', \n",
    "               'identical to d?pfr', 'copied (?:directly )?from', \n",
    "                'direct copy', 'inherited from', 'isa[\\w\\s\\-#]*\\d{4,5}', \n",
    "               'copy from (?:smap )?pfr', 'created twice', 'companion d?pfr', \n",
    "               'pfr[\\w\\s]*carried over from', ]\n",
    "    pfr = pfr[-pfr.Description.str.lower()\\\n",
    "              .str.contains('|'.join(desc_dup_key), na=True)]\n",
    "\n",
    "    pfr['Title'] = pfr['rep_title']\n",
    "    pfr = pfr.drop(columns=['rep_title', 'rep_num'])\n",
    "    return pfr\n",
    "\n",
    "\n",
    "\n",
    "def rm_dup_isa(isa):\n",
    "    # remove dummy test data from PRS 2.0 dev team\n",
    "    dev_team = [43660,75254,75426,76744,78939,80050,82903,83760]\n",
    "    isa = isa[-isa.OriginatorUserID.isin(dev_team)]\n",
    "    isa = isa[-isa.Project_Name.isin(['DHahn Project', 'DHahn1'])]\n",
    "\n",
    "    # separate report numbers from titles that are migrated from legacy system\n",
    "    isa['rep_num'], isa['rep_title'] = isa['Title'].apply(lambda x: get_rep_num(x)).str\n",
    "\n",
    "    # remove duplicates in report description \n",
    "    # Reports may be generated multiple times due to system glitches in PRS\n",
    "    isa = isa[-isa.Description.str.lower().duplicated()]\n",
    "\n",
    "    # use keywords in report titles to remove duplicates\n",
    "    title_dup_key = ['pfr', 'accidental duplicate']\n",
    "    isa = isa[-isa.rep_title.str.lower().str.contains('|'.join(title_dup_key))]\n",
    "\n",
    "    # remove test ISA reports\n",
    "    isa = isa[-(isa.rep_title.str.lower().str.contains('test') & \n",
    "                (isa.Description.str.len() < 30))]\n",
    "\n",
    "    # originally opened as pfr, duplicates, etc\n",
    "    isa = isa[-(isa.Description.notnull() & \n",
    "          isa.Description.str.lower().str.contains('administrative'))]\n",
    "\n",
    "    isa['Title'] = isa['rep_title']\n",
    "    isa = isa.drop(columns=['rep_title', 'rep_num'])\n",
    "    return isa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_dup = {'PFR':rm_dup_pfr, 'DPFR':rm_dup_pfr, 'ISA':rm_dup_isa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_types = df.ReportType.unique()\n",
    "documents_by_type = \\\n",
    "    {t: rm_dup[t](df.loc[df.ReportType==t].dropna(axis=1, how='all'))\n",
    "     for t in report_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in documents_by_type:\n",
    "    documents_by_type[t].to_csv(f'{t}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_by_type = \\\n",
    "    {t:np.array(documents_by_type[t].columns)\n",
    "     for t in report_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type['ISA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find free fields\n",
    "\n",
    "def get_label_diversity(df):\n",
    "    \n",
    "    diversity = np.array(\n",
    "        [df[l].nunique() / df[l].dropna().count() \n",
    "         for l in df.columns]\n",
    "    )\n",
    "    return diversity\n",
    "\n",
    "def get_label_population(df):\n",
    "    rows, cols = df.shape\n",
    "    population = np.array(df.count()/rows)\n",
    "    return population\n",
    "\n",
    "diverse_thresh = .4\n",
    "population_thresh = .1\n",
    "suspects_by_type = dict()\n",
    "\n",
    "for t in report_types:\n",
    "    labels = labels_by_type[t]\n",
    "    documents = documents_by_type[t]\n",
    "    rows, cols = documents.shape    \n",
    "\n",
    "    label_diversity = get_label_diversity(documents)\n",
    "    ddx = (label_diversity > diverse_thresh) #& (label_diversity < 1.0)\n",
    "\n",
    "    label_population = get_label_population(documents)\n",
    "    pdx = label_population > population_thresh\n",
    "\n",
    "    idx = ddx & pdx\n",
    "\n",
    "    suspects_by_type[t] = labels[idx]\n",
    "    print(f'{t} ({rows}) [label, population, diversity]:', *zip(\n",
    "        labels[idx],\n",
    "        np.round(label_population[idx], 3),\n",
    "        np.round(label_diversity[idx], 3)\n",
    "    ), sep='\\n  ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in report_types:\n",
    "    display(documents_by_type[t][suspects_by_type[t]].dropna().head(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored = \\\n",
    "    'Anomaly_ID', 'CountID', 'DateClosed', 'LastProcessed', \\\n",
    "    'OriginationDate', 'ProblemFailureDate', 'FlightSWVersion', \\\n",
    "    'Procedure'\n",
    "\n",
    "targets_by_type = dict()\n",
    "\n",
    "for t in report_types:\n",
    "    suspects = suspects_by_type[t]\n",
    "    targets = suspects[~np.isin(suspects, censored)]\n",
    "    targets_by_type[t] = targets\n",
    "    display(documents_by_type[t][targets].dropna().head(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in documents_by_type:\n",
    "    _ = documents_by_type[t][targets_by_type[t]]\n",
    "    display(f'-- {t} --')\n",
    "    display(\n",
    "        _.applymap(lambda x: len(x.split()) \n",
    "                   if type(x) is str else float('nan')).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCPRE = 'GLOMPRE'\n",
    "DOCPOST = 'GLOMPOST'\n",
    "import string\n",
    "\n",
    "filters = list(read_csv(BASEDIR / 'html_escape_characters.csv').escape_char) + [\n",
    "        r'[\\s]',\n",
    "        r'=[^\\s]*',\n",
    "        r'(\\d+-\\d+t)?\\d+:\\d+:\\d+(.\\d+)?',\n",
    "        r'<.+?>',\n",
    "        r'\\d+\\.\\d+\\.\\d+',\n",
    "        r'(\\()?\\d+/\\d+/\\d+(\\))?',\n",
    "        r'\\s\\d+\\s',\n",
    "        r'\\*',\n",
    "        r'\\d\\d\\d\\d-\\d\\d-\\d\\d',\n",
    "        r'[\\(\\),\\'\\\"\\.:]',\n",
    "        r'\\s.\\s',\n",
    "        r'\\[from.+?\\]',\n",
    "        ',|:|;|\\(|\\)|\\[|\\]|{|}|<|>|\"|=|\\?|/|@',\n",
    "        r' \\d+ ',\n",
    "        r' - ',\n",
    "        r' \\| ',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for f in filters:\n",
    "#    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regprocess(s):\n",
    "    s = ' ' + s.lower() + ' '\n",
    "    \n",
    "    for f in filters:\n",
    "        s = partial(re.sub, f, ' ')(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "for t in documents_by_type:\n",
    "    documents_by_type[t][DOCPRE] = \\\n",
    "        documents_by_type[t][targets_by_type[t]].applymap(str).apply(list, axis=1).apply(' '.join)\n",
    "    documents_by_type[t][DOCPOST] = documents_by_type[t][DOCPRE].apply(regprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('\\s.\\s', ' ', ' + sam + am + ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type[t].GLOMPRE.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type[t].GLOMPOST.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reverstem import PorterStemmer\n",
    "\n",
    "PorterStemmer.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from collections import Counter\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tostems(doc):\n",
    "    return ' '.join(\n",
    "        [x.unit for x in map(stemmer.stem, doc.split()) if x]\n",
    "    )\n",
    "\n",
    "\n",
    "for t in documents_by_type:\n",
    "    documents_by_type[t]['GLOMSTEM'] = documents_by_type[t].GLOMPOST.apply(tostems)\n",
    "\n",
    "\n",
    "lookup = lambda key: stemmer._lookup[key].most_common()[0][0]\n",
    "def unstem(doc):\n",
    "    return ' '.join(\n",
    "        [x for x in map(lookup, doc.split()) if x]\n",
    "    )\n",
    "\n",
    "for t in documents_by_type:\n",
    "    documents_by_type[t]['GLOMUNSTEM'] = documents_by_type[t].GLOMSTEM.apply(unstem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type[t].GLOMUNSTEM.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit(concat(documents_by_type.values()).GLOMSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer._lookup['follow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = np.array(x.get_feature_names())[x.idf_.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer._lookup[stems[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''stems = sorted(\n",
    "    stemmer.rare_stems(10),\n",
    "    key = lambda stem: sum(stemmer._lookup[stem].values()),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "words = [\n",
    "    stemmer._lookup[s].most_common(1)[0][0]\n",
    "    for s in stems\n",
    "    if stemmer._lookup[s].most_common(1)\n",
    "]\n",
    "len(words), len(stems), len(stemmer._lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise \"pause\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reverstem import stopwords as _old_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 160\n",
    "\n",
    "\n",
    "print(top / len(words))\n",
    "tump = set(words[:top]).difference(_old_stopwords)\n",
    "tump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'yourselves' in stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = sum(bool(stemmer._lookup[s]) for s in stems[:top])\n",
    "from collections import ChainMap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ChainMap(*(stemmer._lookup[s] for s in tump)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#raise \"pause\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nltkwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \n",
    "\"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \n",
    "\"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \n",
    "\"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \n",
    "\"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "\"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "\"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \n",
    "\"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \n",
    "\"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "\"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \n",
    "\"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \n",
    "\"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "\"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \n",
    "\"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \n",
    "\"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \n",
    "\"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \n",
    "\"now\"]\n",
    "\n",
    "_stopwords = ChainMap(*(stemmer._lookup[s]\n",
    "                        for s in chain(stems[:top+14], \n",
    "                                       [stemmer.stem(s).unit for s in nltkwords]\n",
    "                                      )\n",
    "                       ))\n",
    "\n",
    "stopwords = sorted(_stopwords.keys())\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_drop_by_type = {'ISA':.2, 'PFR':.1, 'DPFR':.3}\n",
    "idx_drop_by_type = {}\n",
    "for t in documents_by_type:\n",
    "    data = documents_by_type[t][['GLOMUNSTEM', 'GLOMPRE']]\n",
    "    info = data.applymap(\n",
    "        lambda s: len(str(s).split())\n",
    "    )\n",
    "    idx = info.GLOMUNSTEM / info.GLOMPRE < word_drop_by_type[t]\n",
    "    idx_drop_by_type[t] = idx\n",
    "    #print(t, data[idx].shape)\n",
    "    #display(data[idx])\n",
    "    #documents_by_type[t] = documents_by_type[t][~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in documents_by_type:\n",
    "    display(t)\n",
    "    display(list(documents_by_type[t][idx_drop_by_type[t]].Anomaly_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in documents_by_type:\n",
    "    info = documents_by_type[t][['GLOMUNSTEM', 'GLOMPRE']].applymap(\n",
    "        lambda s: len(str(s).split())\n",
    "    )\n",
    "    idx = info.GLOMUNSTEM < 10\n",
    "    print(sum(idx))\n",
    "#    display(documents_by_type[t][idx].GLOMPRE)\n",
    "    documents_by_type[t] = documents_by_type[t][~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Path('./../output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(out / 'stemmer.pyo', 'wb') as fd:\n",
    "    pickle.dump(file=fd, obj=stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for t in documents_by_type:\n",
    "    documents_by_type[t][\n",
    "        ['Anomaly_ID', 'GLOMUNSTEM', 'Project_Code', 'GLOMPRE'] #+ author_columns[:-1]\n",
    "    ].to_csv(out / f'norm_{t}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(out / 'stemmer.pyo', 'rb') as fd:\n",
    "    tump = pickle.load(fd)\n",
    "    \n",
    "tump._lookup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
