{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import isnan\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv, concat, isna\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "%precision 4\n",
    "\n",
    "BASEDIR = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'prs.csv') as fd:\n",
    "    df = read_csv(fd, low_memory=False)\n",
    "\n",
    "with open(BASEDIR / 'prs_signatures.csv') as fd:\n",
    "    ef = read_csv(fd, low_memory=False)\n",
    "df.shape, ef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(isna(ef['Middle_Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_authors = [\n",
    "    43660., 75254., 75426., 76744., 78939., 80050., 82903., 83760., \n",
    "    43660,  75254,  75426,  76744,  78939,  80050,  82903,  83760,\n",
    "]\n",
    "idx = ef.applymap(lambda i: i in censored_authors).any(axis=1)\n",
    "display(ef[idx])\n",
    "ef = ef[~idx]\n",
    "\n",
    "\n",
    "idx = df.applymap(lambda i: i in censored_authors).any(axis=1)\n",
    "display(df[idx])\n",
    "df = df[~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_types = df.ReportType.unique()\n",
    "documents_by_type = \\\n",
    "    {t:df.loc[df.ReportType==t].dropna(axis=1, how='all')\n",
    "     for t in report_types}\n",
    "\n",
    "signers_by_type = \\\n",
    "    {t:ef.loc[ef.ReportType==t].dropna(axis=1, how='all')\n",
    "     for t in report_types}\n",
    "\n",
    "\n",
    "labels_by_type = \\\n",
    "    {t:np.array(documents_by_type[t].columns)\n",
    "     for t in report_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored = [] #\\\n",
    "#    ['AlertConcern', 'CauseCodes', 'DateClosed', \n",
    "#     'Description', 'FailureCause', 'CountID',\n",
    "#     'Disposition', 'ExecutiveSummary', 'GroundSWVersion',\n",
    "#     'FlightSWVersion', \n",
    "#    ]\n",
    "\n",
    "for t in labels_by_type:\n",
    "    candidates = (l for l in labels_by_type[t] if l not in censored)\n",
    "    print(f'{t} ::', *candidates, sep='\\n  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find free fields\n",
    "\n",
    "def get_label_diversity(df):\n",
    "    \n",
    "    diversity = np.array(\n",
    "        [df[l].nunique() / df[l].dropna().count() \n",
    "         for l in df.columns]\n",
    "    )\n",
    "    return diversity\n",
    "\n",
    "def get_label_population(df):\n",
    "    rows, cols = df.shape\n",
    "    population = np.array(df.count()/rows)\n",
    "    return population\n",
    "\n",
    "diverse_upper, diverse_lower = 1., .00\n",
    "population_thresh = .0\n",
    "suspects_by_type = dict()\n",
    "\n",
    "for t in report_types:\n",
    "    labels = labels_by_type[t]\n",
    "    documents = documents_by_type[t]\n",
    "    rows, cols = documents.shape\n",
    "\n",
    "    label_diversity = get_label_diversity(documents)\n",
    "    ddx = (label_diversity < diverse_upper) & (label_diversity > diverse_lower)\n",
    "    label_population = get_label_population(documents)\n",
    "    pdx = label_population > population_thresh\n",
    "\n",
    "    idx = ddx & pdx\n",
    "\n",
    "    suspects_by_type[t] = labels[idx]\n",
    "    #print(f'{t} ({rows}) [label, population, diversity]:', *zip(\n",
    "    #    labels[idx],\n",
    "    #    #np.round(label_population[idx], 3),\n",
    "    #    np.round(label_diversity[idx], 3)\n",
    "    #), sep='\\n  ')\n",
    "    \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in documents_by_type:\n",
    "    print(documents_by_type[t].dtypes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censored = [ # remove columns that are definately not author identifiers\n",
    "    'CorrectiveAction', 'CorrectiveActionHrs', 'DateClosed',\n",
    "    'Description', 'Effectivity', 'FailureEffectRating',\n",
    "    'FailureCause', 'CauseCodes', 'AlertConcern', \n",
    "    'Status', 'FlightSWVersion', 'LastProcessed',\n",
    "    'HardwareSafety', 'Mail_Stop', 'MainItemAffected',\n",
    "    'LessonsLearned', 'MissionCriticalFailure', 'MissionPhaseAffected',\n",
    "    'SpecificEnvironment', 'SafetyReviewStatus', 'SuspectedProblemArea',\n",
    "    'Telephone_Number', 'Title', 'VerificationAnalysis', \n",
    "    'ReportingLocation', 'ReportType', 'REV',\n",
    "    'Project_Name', 'TestVerification', 'ProjectPhase',\n",
    "    'WorkstationName', 'ExecutiveSummary', 'GroundSWVersion',\n",
    "    'OriginationDate', 'SuspectedCause', 'Issues',\n",
    "    'Reproducible', 'CogEClosurePlan', 'assignElement',\n",
    "    'ProblemFailureDate', 'Phase', 'PersonnelSafety',\n",
    "    'ProblemType', 'Procedure', 'ProblemFailureNotedDuring',\n",
    "    'Project_Code', 'Disposition', 'OperatingSystemVersion',\n",
    "    'Paragraph', 'OperatingSystem', 'AnalysisImpacts',\n",
    "    'SystemContractor', 'CmdFileError', 'ResidualRisk',\n",
    "    'SubsystemName', 'Rationale', 'ProgramName',\n",
    "    'anpro', 'Priority', 'MissionActivity',\n",
    "    'Location', 'MissionPhase', 'InitialCritValue',\n",
    "    'InitialCrit', 'ISACauseCodes2016', 'TestResultsVerification',\n",
    "    'VerificationActHours', 'FlightProjectConcurrence', 'MultipleTeams',\n",
    "    'LessonsLearnedCandidate', 'DateOfIncident', 'CritRating',\n",
    "    'CmdFileCategory', 'CmdFileCorrectiveAction', 'CmdFileErrorCauses',\n",
    "    'CmdFileUplinkProcessLocations', 'CmfFileErrorDescription', 'CmfFileRootCause',\n",
    "    'ISACauseCodes', 'DateRequiredBy', 'HWSWItem',\n",
    "    'CommandProcessRelated', 'CmfFileCorrectiveAction', 'CmfFileProximateCause',\n",
    "    'CmfFileContributingCause', 'AdministrativeComment', 'Project_ID'\n",
    "]\n",
    "for t in reversed(list(documents_by_type.keys())):\n",
    "    display(documents_by_type[t].select_dtypes(include=['O']).drop(censored, axis=1, errors='ignore').head(n=50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'ISA'\n",
    "tump = documents_by_type[t].select_dtypes(include=['O']).drop(censored, axis=1, errors='ignore')\n",
    "tump.fullname.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*tump.columns, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "censored = censored + [\n",
    "    'CognizantSection', 'CountID', 'EstimatedHrsToComplete',\n",
    "    'FailureCauseValue', 'FailureEffectRatingValue', 'TestActHours',\n",
    "    'VerificationAnalysisHours', 'CritRatingValue', 'AnalysisImpactsHours'\n",
    "]\n",
    "for t in documents_by_type:\n",
    "    display(documents_by_type[t].select_dtypes(include=numerics).drop(censored, axis=1, errors='ignore').head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in documents_by_type:\n",
    "    display(documents_by_type[t].drop(censored, axis=1, errors='ignore').dropna().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signers_by_type[t].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "authors_by_type = dict()\n",
    "\n",
    "Key = ['Anomaly_ID', 'ResponsibleEditorUserId', 'OriginatorUserID', 'AssigneeUserID']\n",
    "\n",
    "for t in documents_by_type:\n",
    "    authors_by_type[t] = documents_by_type[t][Key].melt(\n",
    "        ['Anomaly_ID'], \n",
    "        var_name='UserRoleName', \n",
    "        value_name='Users_ID'\n",
    "    ).dropna()\n",
    "\n",
    "    authors_by_type[t].Users_ID = authors_by_type[t].Users_ID.astype(int)\n",
    "    authors_by_type[t] = authors_by_type[t].\\\n",
    "        replace('ResponsibleEditorUserId', 'RESPONSIBLE EDITOR').\\\n",
    "        replace('AssigneeUserID', 'ASSIGNEE').\\\n",
    "        replace('OriginatorUserID', 'ORIGINATOR')\n",
    "\n",
    "    authors_by_type[t] = \\\n",
    "        concat([authors_by_type[t], signers_by_type[t][['Anomaly_ID', 'Users_ID', 'UserRoleName']]], sort=True).\\\n",
    "        sort_values(['Users_ID', 'Anomaly_ID'])\n",
    "    \n",
    "    authors_by_type[t] = \\\n",
    "        authors_by_type[t][ # as informed by Bruce\n",
    "          (authors_by_type[t]['UserRoleName'] != 'DEVELOPER') &\n",
    "          (authors_by_type[t]['UserRoleName'] != 'EDITOR') &\n",
    "          (authors_by_type[t]['UserRoleName'] != 'ORIGINATOR') &\n",
    "          (authors_by_type[t]['UserRoleName'] != 'CONDITIONAL APPROVER')\n",
    "        ]\n",
    "    \n",
    "    authors_by_type[t]['_Anomaly_ID'] = 'A' + authors_by_type[t]['Anomaly_ID'].apply(str)\n",
    "    authors_by_type[t]['_Users_ID'] = 'U' + authors_by_type[t]['Users_ID'].apply(str)\n",
    "    \n",
    "    # XXX PMR: Apparently there are a lot of duplicate assignments. This now\n",
    "    #  ignores the 'UserRoleName'\n",
    "    #authors_by_type[t].drop_duplicates(['Anomaly_ID', 'Users_ID'], inplace=True)\n",
    "\n",
    "    print(\n",
    "        t, \n",
    "        f'AUTHORS   - {authors_by_type[t].shape[0]}', \n",
    "        f'ANOMALIES - {authors_by_type[t].Anomaly_ID.unique().shape[0]}',\n",
    "        sep='\\n  :'\n",
    "    )\n",
    "    print(\n",
    "        'Author Types', \n",
    "        *authors_by_type[t].UserRoleName.unique(),\n",
    "        sep='\\n  :'\n",
    "    )\n",
    "    display(authors_by_type[t].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in authors_by_type:\n",
    "    authors_by_type[t][['Anomaly_ID', 'UserRoleName', 'Users_ID']].to_csv(f'{t}_experts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_DOCUMENTS = 0\n",
    "\n",
    "for document_type in authors_by_type:\n",
    "    count_authors = authors_by_type[document_type]['Users_ID']\n",
    "    sufficient = {\n",
    "        author\n",
    "        for author, count in Counter(count_authors).items()\n",
    "        if count >= MINIMUM_DOCUMENTS\n",
    "    }\n",
    "    idx = authors_by_type[document_type]['Users_ID'].isin(sufficient)\n",
    "    authors_by_type[document_type] = authors_by_type[document_type][idx]\n",
    "\n",
    "    print(\n",
    "        document_type, \n",
    "        f'AUTHORS   - {authors_by_type[document_type].shape[0]}', \n",
    "        f'ANOMALIES - {authors_by_type[document_type].Anomaly_ID.unique().shape[0]}',\n",
    "        sep='\\n  :'\n",
    "    )\n",
    "    print(\n",
    "        'Author Types', \n",
    "        *authors_by_type[t].UserRoleName.unique(),\n",
    "        sep='\\n  :'\n",
    "    )\n",
    "    display(authors_by_type[t].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in authors_by_type:\n",
    "    authors_by_type[t]['ReportType'] = t\n",
    "\n",
    "concat(authors_by_type.values()).to_csv(BASEDIR / 'processed_authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in authors_by_type:\n",
    "    print(t, len(authors_by_type[t].Anomaly_ID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import product\n",
    "\n",
    "graph_by_type = {}\n",
    "\n",
    "for t in authors_by_type:\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        authors_by_type[t], \n",
    "        edge_attr='UserRoleName', \n",
    "        source='_Users_ID', \n",
    "        target='_Anomaly_ID').to_undirected()\n",
    "\n",
    "    #display(nx.draw(G, with_labels=False, node_size=1))\n",
    "\n",
    "\n",
    "    for a in authors_by_type[t]['_Anomaly_ID'].unique():\n",
    "        users = list(nx.all_neighbors(G, a))\n",
    "        G.add_edges_from(product(users, users))\n",
    "\n",
    "    for a in authors_by_type[t]['_Anomaly_ID'].unique():\n",
    "        G.remove_node(a)\n",
    "        \n",
    "    graph_by_type[t] = G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in graph_by_type:\n",
    "    _ = plt.figure()\n",
    "    nx.draw(graph_by_type[t], node_size=1, title=t)\n",
    "    plt.title(t)\n",
    "    plt.savefig(f'{t}_authorship_graph.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
