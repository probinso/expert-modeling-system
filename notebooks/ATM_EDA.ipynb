{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, Counter\n",
    "from glob import glob\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from os import remove\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel as LDA\n",
    "from gensim.models import AuthorTopicModel as ATM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "\n",
    "import pyLDAvis as ldavis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import coo_matrix as sparse_matrix\n",
    "\n",
    "ldavis.enable_notebook()\n",
    "%matplotlib notebook\n",
    "#%precision 4\n",
    "\n",
    "\n",
    "Candidate = namedtuple('Candidate', ['iterations', 'num_topics'])\n",
    "\n",
    "\n",
    "\n",
    "def get_i_t(filename):\n",
    "    _, content, document_type = filename.split('-')\n",
    "    i, t, _ = content.split('_')\n",
    "    return int(i[1:]), int(t[1:]), document_type.split('.')[0]\n",
    "\n",
    "get_texts = lambda df: df[target].str.split()\n",
    "tobows = lambda df, d: concat([df['Anomaly_ID'], get_texts(df).apply(d.doc2bow)], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "isr2 = 2.0 ** -.5\n",
    "\n",
    "def hellinger(x, y):\n",
    "    return isr2 * np.sqrt(((np.sqrt(x) - np.sqrt(y)) ** 2).sum())\n",
    "\n",
    "\n",
    "\n",
    "report_types = 'ISA', 'PFR', 'DPFR'\n",
    "\n",
    "TEST_SIZE = 0.3\n",
    "min_occurances = 2\n",
    "target = 'GLOMUNSTEM'\n",
    "\n",
    "%matplotlib notebook\n",
    "%precision 4\n",
    "\n",
    "BASEDIR = Path('../data')\n",
    "OUT = Path('../output/atm/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'processed_authors.csv') as fd:\n",
    "    af = read_csv(fd)\n",
    "af.shape\n",
    "af.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_type = {\n",
    "    t: af[af.ReportType == t] \n",
    "    for t in af.ReportType.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type = {\n",
    "    t: read_csv(OUT / f'../norm_{t}.csv').dropna()\n",
    "    for t in af.ReportType.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_document_downselect(documents, authors, ANOMALY_LABEL='Anomaly_ID'):\n",
    "    documents[ANOMALY_LABEL] = \\\n",
    "      'A' + documents[ANOMALY_LABEL].apply(str)\n",
    "\n",
    "    idx = documents[ANOMALY_LABEL].isin(\n",
    "        authors[ANOMALY_LABEL].unique()\n",
    "    )\n",
    "\n",
    "    documents = documents[idx]\n",
    "    \n",
    "    idx = authors[ANOMALY_LABEL].isin(\n",
    "        documents[ANOMALY_LABEL].unique()\n",
    "    )\n",
    "    authors = authors[idx]\n",
    "    return documents, authors\n",
    "\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    documents_by_type[document_type], authors_by_type[document_type] = \\\n",
    "        author_document_downselect(\n",
    "          documents_by_type[document_type],\n",
    "          authors_by_type[document_type]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type['ISA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_and_vocab(documents, test_size, min_occurances=min_occurances):\n",
    "    train, test = train_test_split(\n",
    "        documents, test_size=test_size\n",
    "    )\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    vocab = Dictionary(\n",
    "        train[target].str.split()\n",
    "    )\n",
    "    vocab.filter_extremes(no_below=min_occurances)\n",
    "\n",
    "    return train, test, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_by_type, train_documents_by_type, test_documents_by_type, dictionary_by_type = \\\n",
    "  dict(), dict(), dict(), dict()\n",
    "\n",
    "train_author_table_by_type = dict()\n",
    "test_author_table_by_type = dict()\n",
    "\n",
    "def attribution_table(documents, relevent_authors):\n",
    "    store = defaultdict(set)\n",
    "    for idx, anomaly in documents.iterrows():\n",
    "        authors_documents = relevent_authors[\n",
    "            relevent_authors.Anomaly_ID == anomaly.Anomaly_ID\n",
    "        ]\n",
    "\n",
    "        for author in authors_documents.Users_ID:\n",
    "            store[author].add(idx)\n",
    "\n",
    "    return {k: list(v) for k, v in store.items()}\n",
    "\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    train_documents_by_type[document_type], \\\n",
    "    test_documents_by_type[document_type], \\\n",
    "    dictionary_by_type[document_type] = \\\n",
    "        get_train_test_and_vocab(documents_by_type[document_type], TEST_SIZE)\n",
    "\n",
    "    train_author_table_by_type[document_type] = attribution_table(\n",
    "        train_documents_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )\n",
    "\n",
    "    test_author_table_by_type[document_type] = attribution_table(\n",
    "        test_documents_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )\n",
    "\n",
    "    print(f'{document_type: <4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise \"pause\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob(str(OUT / 'wide-*.')):\n",
    "    remove(filename)\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    corpus = tobows(\n",
    "        train_documents_by_type[document_type],\n",
    "        dictionary_by_type[document_type]\n",
    "    )[target]\n",
    "\n",
    "    for iterations in range(1, 352, 50):\n",
    "        print(f'{document_type: <4}', f'{iterations:03}', end=' - ')\n",
    "        for num_topics in range(1, 122, 3):\n",
    "\n",
    "            model = ATM(corpus=list(corpus),\n",
    "                        author2doc=train_author_table_by_type[document_type],\n",
    "                        num_topics=num_topics,\n",
    "                        iterations=iterations,\n",
    "                       )\n",
    "            print(num_topics, end=':')\n",
    "\n",
    "            savename = f'wide-i{iterations:03}_t{num_topics:03}_d-{document_type}'\n",
    "            model.save(str(OUT / f'{savename}.atm'))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_coherence(filename, coherence_type):\n",
    "    iterations, num_topics, document_type = get_i_t(filename)\n",
    "\n",
    "    # holdout not used intentionally\n",
    "    corpus = tobows(\n",
    "        train_documents_by_type[document_type],\n",
    "        dictionary_by_type[document_type]\n",
    "    )[target]\n",
    "\n",
    "    model = ATM.load(filename)\n",
    "    cm = CoherenceModel(\n",
    "        model=model,\n",
    "        corpus=corpus,\n",
    "        texts=train_documents_by_type[document_type][target].apply(str.split),\n",
    "        dictionary=dictionary_by_type[document_type],\n",
    "        coherence=coherence_type\n",
    "    )\n",
    "\n",
    "    cm.save(filename.replace('.atm', f'.cm.{coherence_type}'))\n",
    "    return cm\n",
    "\n",
    "\n",
    "coherence_labels = ['c_v', 'u_mass']\n",
    "\n",
    "for coherence_type in coherence_labels:\n",
    "    print(coherence_type)\n",
    "    for filename in glob(str(OUT / 'wide-*.atm')):\n",
    "        build_coherence(filename, coherence_type)\n",
    "        print('*', end='')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(filename, coherence_labels):\n",
    "    iterations, num_topics, document_type = get_i_t(filename)\n",
    "\n",
    "    train_corpus = tobows(\n",
    "        train_documents_by_type[document_type], \n",
    "        dictionary_by_type[document_type]\n",
    "    )[target]\n",
    "\n",
    "    test_corpus = tobows(\n",
    "        test_documents_by_type[document_type],\n",
    "        dictionary_by_type[document_type]\n",
    "    )[target]\n",
    "\n",
    "    atm = ATM.load(filename)\n",
    "    coherences = [\n",
    "        CoherenceModel.load(\n",
    "            filename.replace('.atm', f'.cm.{ct}')\n",
    "        ).get_coherence()\n",
    "        for ct in coherence_labels\n",
    "    ]\n",
    "\n",
    "    #p_train = model.log_perplexity(train_corpus)\n",
    "    #p_test = model.log_perplexity(test_corpus)\n",
    "\n",
    "    row = [\n",
    "        document_type, num_topics, iterations,\n",
    "        #p_train, p_test,\n",
    "    ] + coherences\n",
    "    return row\n",
    "\n",
    "acc = []\n",
    "for filename in glob(str(OUT / 'wide-*.atm')):\n",
    "    row = get_metrics(filename, coherence_labels)\n",
    "    acc.append(row)\n",
    "    print('*', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wdiagnosis = DataFrame(\n",
    "    acc,\n",
    "    columns=[\n",
    "        'document_type', 'num_topics', 'iterations', \n",
    "        #'train_perplexity', 'test_perplexity', \n",
    "    ] + coherence_labels\n",
    ")\n",
    "wdiagnosis.to_csv('./saveout.csv', index=False)\n",
    "plt.scatter(wdiagnosis.num_topics, wdiagnosis.u_mass, c=wdiagnosis.iterations)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdiagnosis = DataFrame.from_csv('./saveout.csv', index_col=None)\n",
    "metric = 'c_v'\n",
    "for document_type in wdiagnosis.document_type.unique():\n",
    "    __ = plt.figure()\n",
    "    _ = sns.boxplot(\n",
    "        x=\"num_topics\", y=metric,\n",
    "        data=wdiagnosis[\n",
    "            (wdiagnosis.document_type == document_type) &\n",
    "            #(wdiagnosis.iterations > 50) &\n",
    "            (wdiagnosis.num_topics < 100) &\n",
    "            True\n",
    "        ],\n",
    "        palette=\"Set3\")\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.title(document_type)\n",
    "    plt.savefig(f'{document_type}_{metric}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document_type in wdiagnosis.document_type.unique():\n",
    "    display(document_type,\n",
    "            wdiagnosis[\n",
    "                (wdiagnosis.document_type==document_type) &\n",
    "                (wdiagnosis.num_topics == 25) &\n",
    "                True\n",
    "            ].sort_values(['c_v'], ascending=False).head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise \"pause\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_by_type = {\n",
    "    'ISA' : Candidate(iterations=351, num_topics=40),\n",
    "    'PFR' : Candidate(iterations=251, num_topics=25),\n",
    "    'DPFR': Candidate(iterations=351, num_topics=25),\n",
    "}\n",
    "\n",
    "for document_type in election_by_type:\n",
    "    c = election_by_type[document_type]\n",
    "\n",
    "    corpus = tobows(\n",
    "        train_documents_by_type[document_type],\n",
    "        dictionary_by_type[document_type]\n",
    "    )[target]\n",
    "\n",
    "    atm = ATM(\n",
    "        corpus=list(corpus),\n",
    "        author2doc=train_author_table_by_type[document_type],\n",
    "        num_topics=c.num_topics,\n",
    "        iterations=c.iterations,\n",
    "    )\n",
    "    model_by_type[document_type] = atm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topics(model, doc_bow):\n",
    "\n",
    "    gamma_chunk, sstats = model.inference(\n",
    "        chunk=[doc_bow], author2doc=dict(), doc2author=dict(), \n",
    "        rhot=1.00,\n",
    "        collect_sstats=True\n",
    "    )\n",
    "\n",
    "    return gamma_chunk\n",
    "\n",
    "\n",
    "def get_model_author_topic_vectors(model):\n",
    "    author_topic_vectors = np.zeros(\n",
    "        (model.num_authors, model.num_topics)\n",
    "    )\n",
    "\n",
    "    for i, author in enumerate(model.id2author.values()):\n",
    "        idx, scores = zip(*model.get_author_topics(author))\n",
    "        author_topic_vectors[i, idx] = scores\n",
    "\n",
    "    return author_topic_vectors\n",
    "\n",
    "\n",
    "def get_sorted_authors(model, doc_bow, author_topic_vectors, metric=hellinger):\n",
    "    doc_vector = get_document_topics(model, doc_bow)\n",
    "\n",
    "    author_scores = np.argsort(\n",
    "        cdist(doc_vector, author_topic_vectors, metric=metric)\n",
    "    )\n",
    "\n",
    "    contenders = [\n",
    "        model.id2author[idx]\n",
    "        for idx in author_scores[0]\n",
    "    ]\n",
    "\n",
    "    return contenders\n",
    "\n",
    "\n",
    "def get_all_ranks_and_counts(model, publication_counts, authors, dictionary, documents):\n",
    "    author_topic_vectors = get_model_author_topic_vectors(model)\n",
    "\n",
    "    for idx, row in tobows(documents, dictionary).iterrows():\n",
    "        Anomaly_ID, doc_bow = row['Anomaly_ID'], row[target]\n",
    "\n",
    "        contenders = get_sorted_authors(model, doc_bow, author_topic_vectors)\n",
    "        \n",
    "        real_authors = [\n",
    "            a\n",
    "            for a in authors\n",
    "            if idx in authors[a]\n",
    "        ]\n",
    "\n",
    "        for a in real_authors:\n",
    "            try:\n",
    "                rank = contenders.index(a)\n",
    "            except ValueError as e:\n",
    "                continue   \n",
    "\n",
    "            try:\n",
    "                publications = publication_counts[a]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            yield publications, rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_author_table_by_type\n",
    "\n",
    "TT = 'ISA'\n",
    "\n",
    "publication_counts_by_type = dict()\n",
    "for t in train_author_table_by_type:\n",
    "    _authors = train_author_table_by_type[t]\n",
    "    publication_counts_by_type[t] = {a: len(_authors[a]) for a in _authors}\n",
    "    del _authors\n",
    "\n",
    "train = list(get_all_ranks_and_counts(\n",
    "    model_by_type[TT],\n",
    "    publication_counts_by_type[TT],\n",
    "    train_author_table_by_type[TT],\n",
    "    dictionary_by_type[TT],\n",
    "    train_documents_by_type[TT]\n",
    "))\n",
    "\n",
    "test = list(get_all_ranks_and_counts(\n",
    "    model_by_type[TT],\n",
    "    publication_counts_by_type[TT],\n",
    "    test_author_table_by_type[TT],\n",
    "    dictionary_by_type[TT],\n",
    "    test_documents_by_type[TT]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_documents_by_type[TT].shape)\n",
    "print(train_documents_by_type[TT].iloc[0][target])\n",
    "\n",
    "from itertools import chain\n",
    "#set(chain(*train_author_table_by_type['ISA'].values()))\n",
    "#set(chain(*test_author_table_by_type['ISA'].values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arc(arc):\n",
    "    _ = plt.figure()\n",
    "    # Load the dataset\n",
    "    data = DataFrame(columns=[\"publications\", \"rank\"], data=arc)\n",
    "    _ = sns.jointplot(x=data['publications'], y=data['rank'], \n",
    "                      kind='hex', \n",
    "                      xlim=(0, 250), ylim=(0, 300), gridsize=20\n",
    "                     )\n",
    "\n",
    "plot_arc(train)\n",
    "plot_arc(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(arc, label):\n",
    "    _ = plt.figure()\n",
    "    # Load the dataset\n",
    "    ranks = np.array(arc, dtype=np.float)[:,1]\n",
    "    #print(ranks)\n",
    "    \n",
    "    def get_stats(ranks):\n",
    "        for cap in range(500):\n",
    "            # there are no negatives\n",
    "            idx = ranks < cap\n",
    "            tp = np.sum(idx)\n",
    "            fp = np.sum(ranks[idx])\n",
    "            fn = np.sum(~idx)\n",
    "            tn = np.sum(ranks[~idx])\n",
    "            accuracy = np.sum(idx) / len(idx)\n",
    "\n",
    "            sensitivity = tp / fn\n",
    "            specificity = tn / (fp + tn)\n",
    "            yield cap, accuracy\n",
    "    x, y = zip(*list(get_stats(ranks)))\n",
    "    plt.scatter(x, y)\n",
    "    plt.title(label)\n",
    "    plt.xlabel('cap')\n",
    "    #plt.xlim((0, 10))\n",
    "    plt.ylabel('accuracy')\n",
    "\n",
    "plot(train, f'{TT}-train')\n",
    "plot(test, f'{TT}-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    document_type: ldavis.gensim.prepare(\n",
    "        model_by_type[document_type],\n",
    "        corpus=tobows(\n",
    "            documents_by_type[document_type], \n",
    "            dictionary_by_type[document_type])[target],\n",
    "        dictionary=dictionary_by_type[document_type],\n",
    "    )\n",
    "    for document_type in election_by_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report_type in election_by_type:\n",
    "    display(prepared_data[report_type])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
