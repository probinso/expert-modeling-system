{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, Counter\n",
    "from glob import glob\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel as LDA\n",
    "from gensim.models import AuthorTopicModel as ATM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "\n",
    "import pyLDAvis as ldavis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy\n",
    "\n",
    "ldavis.enable_notebook()\n",
    "%matplotlib notebook\n",
    "#%precision 4\n",
    "\n",
    "out = Path('../output/')\n",
    "\n",
    "Candidate = namedtuple('Candidate', ['iterations', 'num_topics'])\n",
    "\n",
    "def get_i_t(filename):\n",
    "    _, content, document_type = filename.split('-')\n",
    "    i, t, _ = content.split('_')\n",
    "    return int(i[1:]), int(t[1:]), document_type.split('.')[0]\n",
    "\n",
    "get_texts = lambda df: df[target].str.split()\n",
    "tobows = lambda df, d: get_texts(df).apply(d.doc2bow)\n",
    "\n",
    "report_types = 'ISA', 'PFR', 'DPFR'\n",
    "\n",
    "test_size = .2\n",
    "min_occurances = 2\n",
    "target = 'GLOMUNSTEM'\n",
    "\n",
    "%matplotlib notebook\n",
    "%precision 4\n",
    "\n",
    "BASEDIR = Path('../data')\n",
    "OUT = Path('../output/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(BASEDIR / 'processed_authors.csv') as fd:\n",
    "    af = read_csv(fd)\n",
    "af.shape\n",
    "af.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_type = {t: af[af.ReportType == t] for t in af.ReportType.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type = {\n",
    "    t: read_csv(OUT / f'norm_{t}.csv').dropna()\n",
    "    for t in af.ReportType.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'ISA'\n",
    "documents_by_type[t].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_type[t].Anomaly_ID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_by_type[t].Anomaly_ID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document_type in documents_by_type:\n",
    "    documents_by_type[document_type].Anomaly_ID = \\\n",
    "      'A' + documents_by_type[document_type].Anomaly_ID.apply(str)\n",
    "\n",
    "    \n",
    "    idx = documents_by_type[document_type].Anomaly_ID.isin(\n",
    "        authors_by_type[document_type].Anomaly_ID.unique()\n",
    "    )\n",
    "\n",
    "    documents_by_type[document_type] = documents_by_type[document_type][idx]\n",
    "    \n",
    "    idx = authors_by_type[document_type].Anomaly_ID.isin(\n",
    "        documents_by_type[document_type].Anomaly_ID.unique()\n",
    "    )\n",
    "    authors_by_type[document_type] = authors_by_type[document_type][idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document_type in documents_by_type:\n",
    "    print(document_type, len(documents_by_type[document_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_type, test_by_type, dictionary_by_type = dict(), dict(), dict()\n",
    "\n",
    "for document_type in documents_by_type:\n",
    "    \n",
    "    train_by_type[document_type], test_by_type[document_type] = \\\n",
    "      train_test_split(\n",
    "        documents_by_type[document_type], test_size=test_size\n",
    "      )\n",
    "\n",
    "    dictionary_by_type[document_type] = Dictionary(\n",
    "        train_by_type[document_type][target].str.split()\n",
    "    )\n",
    "    dictionary_by_type[document_type].filter_extremes(no_below=min_occurances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('election_by_type.pkl', 'rb') as fd:\n",
    "    election_by_type = pickle.load(fd)\n",
    "\n",
    "model_by_type, train_by_type, test_by_type, dictionary_by_type = \\\n",
    "  dict(), dict(), dict(), dict()\n",
    "\n",
    "train_author_table_by_type = dict()\n",
    "test_author_table_by_type = dict()\n",
    "\n",
    "def attribution_table(documents, relevent_authors):\n",
    "    store = defaultdict(set)\n",
    "    for idx, anomaly in documents.iterrows():\n",
    "        authors_documents = relevent_authors[\n",
    "            relevent_authors.Anomaly_ID == anomaly.Anomaly_ID\n",
    "        ]\n",
    "\n",
    "        for author in authors_documents.Users_ID:\n",
    "            store[author].add(idx)\n",
    "\n",
    "    return {k: list(v) for k, v in store.items()}\n",
    "\n",
    "\n",
    "for document_type in election_by_type:\n",
    "    c = election_by_type[document_type]\n",
    "\n",
    "    dictionary_by_type[document_type] = Dictionary(\n",
    "        documents_by_type[document_type][target].str.split()\n",
    "    )\n",
    "\n",
    "    print(f'{document_type: <4}')\n",
    "\n",
    "    train_by_type[document_type], test_by_type[document_type] = \\\n",
    "      train_test_split(\n",
    "        documents_by_type[document_type], test_size=test_size\n",
    "      )\n",
    "    train_by_type[document_type] = train_by_type[document_type].reset_index(drop=True)\n",
    "    test_by_type[document_type] = test_by_type[document_type].reset_index(drop=True)\n",
    "\n",
    "    dictionary_by_type[document_type].filter_extremes(no_below=min_occurances)\n",
    "\n",
    "    train_author_table_by_type[document_type] = attribution_table(\n",
    "        train_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )\n",
    "    \n",
    "    test_author_table_by_type[document_type] = attribution_table(\n",
    "        test_by_type[document_type],\n",
    "        authors_by_type[document_type]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document_type in election_by_type:\n",
    "    c = election_by_type[document_type]\n",
    "\n",
    "    corpus = tobows(\n",
    "        train_by_type[document_type], \n",
    "        dictionary_by_type[document_type]\n",
    "    )\n",
    "\n",
    "    atm = ATM(\n",
    "        corpus=list(corpus),\n",
    "        author2doc=train_author_table_by_type[document_type],\n",
    "        num_topics=c.num_topics,\n",
    "        iterations=c.iterations,\n",
    "    )\n",
    "    model_by_type[document_type] = atm\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_by_type['ISA'].get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_type['ISA'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_candidates = dict()\n",
    "count_document_ids = Counter()\n",
    "for user_id in train_author_table_by_type['ISA']:\n",
    "    if user_id in test_author_table_by_type['ISA']:\n",
    "        author_candidates[user_id] = test_author_table_by_type['ISA'][user_id]\n",
    "        count_document_ids.update(author_candidates[user_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_document_ids.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise \"pause\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 285\n",
    "test_by_type['ISA'].iloc[doc_id].GLOMUNSTEM.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = dictionary_by_type['ISA'].doc2bow(\n",
    "    test_by_type['ISA'].iloc[doc_id].GLOMUNSTEM.split()\n",
    ")\n",
    "doc\n",
    "\n",
    "isa_atm = model_by_type['ISA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_topic_vectors = np.zeros(\n",
    "    (isa_atm.num_authors, isa_atm.num_topics)\n",
    ")\n",
    "\n",
    "for i, author in enumerate(isa_atm.id2author.values()):\n",
    "    idx, scores = zip(*isa_atm.get_author_topics(author))\n",
    "    author_topic_vectors[i, idx] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma_chunk, sstats = isa_atm.inference(\n",
    "    chunk=[doc], author2doc=dict(), doc2author=dict(), \n",
    "    rhot=1.00,\n",
    "    collect_sstats=True\n",
    ")\n",
    "\n",
    "doc_topics = gamma_chunk / gamma_chunk.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "isr2 = 2.0 ** -.5\n",
    "\n",
    "def hellinger(x, y):\n",
    "    return isr2 * np.sqrt(((np.sqrt(x) - np.sqrt(y)) ** 2).sum())\n",
    "\n",
    "author_scores = np.argsort(\n",
    "    cdist(gamma_chunk, author_topic_vectors, metric=hellinger)\n",
    ")\n",
    "\n",
    "top_k = 200\n",
    "contenders = [\n",
    "    isa_atm.id2author[idx]\n",
    "    for idx in author_scores[0,0:top_k]]\n",
    "\n",
    "print(*contenders[:10], sep='\\n')\n",
    "print(len(contenders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "for idx, author in enumerate(contenders):\n",
    "    if author in test_author_table_by_type['ISA']:\n",
    "        if doc_id in test_author_table_by_type['ISA'][author]:\n",
    "            print(idx)\n",
    "            break\n",
    "        else:\n",
    "            print('-', end='')\n",
    "\n",
    "    else:\n",
    "        print('*', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
