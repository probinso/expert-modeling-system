{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "#NASA color palette\n",
    "nasa = {'red':'#fc3d21','blue':'#0b3d91','grey':'#79797c','black':'#000000'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PRS pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs = pd.read_pickle(\"../../General/Data/prs_index.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import HTML escape characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_escape_chars = pd.read_csv('../Data/html_escape_characters.csv')\n",
    "re_html_escape_chars = \"|\".join(list(df_html_escape_chars['escape_char']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeform text columns that potentially contain \"safing\"\n",
    "cols_ff_text = ['title',\n",
    "                'description',\n",
    "                'correctiveAction',\n",
    "                'verificationAnalysis',\n",
    "                'issues',\n",
    "#                 'relatedDocuments',\n",
    "#                 'analysisImpacts',\n",
    "#                 'attachedFiles',\n",
    "                'testVerification',\n",
    "                'executiveSummary',\n",
    "#                 'procedure',\n",
    "#                 'rev',\n",
    "#                 'cogEClosurePlan',\n",
    "#                 'paragraph',\n",
    "#                 'rationale',\n",
    "#                 'cmfFileErrorDescription',\n",
    "#                 'cmfFileContributingCause',\n",
    "#                 'cmfFileProximateCause',\n",
    "#                 'cmfFileCorrectiveAction',\n",
    "#                 'cmfFileRootCause'\n",
    "               ]\n",
    "\n",
    "# cols_ff_text = ['title','description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and clean the free-form text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text_fields(row):\n",
    "    #Join all text in a report into a single string\n",
    "    text = ''\n",
    "    for col in cols_ff_text:\n",
    "        try:\n",
    "            text += ' ' + row[col]\n",
    "        except TypeError:\n",
    "            continue\n",
    "    return text\n",
    "\n",
    "def clean_text(row):\n",
    "    #Remove HTML escape charactes\n",
    "    text = re.sub(re_html_escape_chars, ' ', row[col_name])\n",
    "\n",
    "    #Push text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #Replace any punctuation, special characters, etc. with whitespace     \n",
    "    text = re.sub('[^A-Za-z0-9]',' ',text)\n",
    "    \n",
    "    #Split text based on whitespace\n",
    "    text = text.split()\n",
    "    \n",
    "    #Keep alphanumerics\n",
    "    text = [re.sub('^(?=[0-9]).*', ' ', word) for word in text]\n",
    "\n",
    "    #Remove subjectively small text\n",
    "    text = [word for word in text if len(word) > 2 and not word.isspace()]\n",
    "\n",
    "    #Remove stopwords\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    \n",
    "    #Set stemmer and use it to stem individual text\n",
    "#     st = LancasterStemmer()\n",
    "    st = PorterStemmer()\n",
    "    text = [st.stem(word) for word in text]\n",
    "    \n",
    "    #Return a string of cleaned text\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "#Apply to main subset\n",
    "# cols_ff_text = ['title','description']\n",
    "col_name = 'text'\n",
    "df_prs[col_name] = df_prs.apply(combine_text_fields, axis=1)\n",
    "df_prs['{0}_cleaned'.format(col_name)] = df_prs.apply(clean_text, axis=1)\n",
    "\n",
    "\n",
    "df_prs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save cleaned text\n",
    "# df_prs[['anomalyID','text_cleaned']].to_csv('../../General/Data/text_cleaned_all_ff_text_fields_all_reports.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load save cleaned text\n",
    "# df_text_cleaned = pd.read_csv('../../General/Data/text_cleaned_all_ff_text_fields_all_reports.csv')\n",
    "\n",
    "df_text_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_prs.shape\n",
    "print df_text_cleaned.shape\n",
    "\n",
    "df_prs = pd.merge(left=df_prs,right=df_text_cleaned,how='left',left_on='anomalyID',right_on='anomalyID')\n",
    "df_prs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "#Print some output\n",
    "# df_prs[['title','description','words']]\n",
    "# df_prs[['fullname',\"responsibleEditorFullName\",\"responsibleEditorUserName\",\"assigneeFullName\",'description','words']]\n",
    "df_prs[['text_cleaned']].head()\n",
    "df_prs.ix[df_prs['anomalyID']==6856,['text','text_cleaned']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_cosine_matrix(report_type, max_n_gram):\n",
    "    #Subset data\n",
    "    df_modeling = df_prs[df_prs['reportType']==report_type].copy()\n",
    "    df_modeling.reset_index(inplace=True)\n",
    "\n",
    "    #Get term frequencies inverse document frequencies\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, max_n_gram))\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df_modeling['text_cleaned'])\n",
    "\n",
    "    print \"Unique terms:\\t{0}\".format(len(tfidf_vectorizer.get_feature_names()))\n",
    "    \n",
    "    return (tfidf * tfidf.T).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix encoding problem\n",
    "def change_text(row):\n",
    "    try:\n",
    "        return row[col].encode('utf-8')\n",
    "    except AttributeError:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def top_n_most_similar(anomaly_id, tf_idf_cosine_dist_matix, n_reports, export_bool=False):\n",
    "    #Get report type\n",
    "    report_type = df_prs.ix[df_prs['anomalyID']==anomaly_id,'reportType'].values[0]\n",
    "    \n",
    "    #Subset data\n",
    "    df_modeling = df_prs[df_prs['reportType']==report_type].copy()\n",
    "    df_modeling.reset_index(inplace=True)\n",
    "    \n",
    "    #Get the index associated with the provided anomaly id\n",
    "    anomaly_id_index = df_modeling[df_modeling['anomalyID']==anomaly_id].index[0]\n",
    "    \n",
    "    #Create df from matrix row associated with anomaly id\n",
    "    df_tf_idf_cosine = pd.DataFrame(tf_idf_cosine_dist_matix[anomaly_id_index],columns=['cosine_dist'])\n",
    "    \n",
    "    #Get top-n distance including the anomaly id in question\n",
    "    top_n = df_tf_idf_cosine.sort_values('cosine_dist',ascending=False).head(n_reports + 1)\n",
    "\n",
    "    #Merge original PFR data to top-n reports\n",
    "    df_out = pd.merge(left=top_n, right=df_modeling, how='left', left_index=True, right_index=True)\n",
    "#     df_out = df_out[['anomalyID','cosine_dist','projectName','title']]\n",
    "    \n",
    "    #Create Excel hyperlink \n",
    "    base_url = 'prs.jpl.nasa.gov/view_anomaly.asp?smode=pop&iAnomalyID='\n",
    "    if report_type == \"ISA\":\n",
    "        base_url = \"https://prs.jpl.nasa.gov/NET/ISAReadOnly.aspx?smode=pop&iAnomalyID=\"\n",
    "    elif report_type == \"PFR\":\n",
    "        base_url = \"https://prs.jpl.nasa.gov/NET/PFRReadOnly.aspx?smode=pop&iAnomalyID=\"\n",
    "    df_out['ID'] = df_out.apply(lambda row: '=hyperlink(\"{0}{1}\",\"{1}\")'.format(base_url,row['anomalyID']), axis=1)\n",
    "    \n",
    "    #Reset index\n",
    "    df_out = df_out.reset_index(drop=True)\n",
    "#     df_out = df_out.reset_index(drop=True)\n",
    "#     df_out = df_out.rename(columns={\"index\":\"rank\"})\n",
    "    \n",
    "#     col = 'title'\n",
    "#     df_out[col] = df_out.apply(change_text, axis=1)\n",
    "    \n",
    "    if export_bool:\n",
    "        #Rename columns\n",
    "        df_out = df_out.rename(columns={'cosine_dist':'Cosine Distance','projectName':'Project Name','title':'Title'})\n",
    "        #Create final df and save it\n",
    "        df_out = df_out[['ID','Cosine Distance','Project Name','Title']]\n",
    "        df_out.to_csv('../Reports/{0}/anomaly_{1}_top_{2}.csv'.format(report_type, anomaly_id, n_reports), index=True, encoding='utf-8')\n",
    "    else:\n",
    "        return df_out[['anomalyID','cosine_dist','projectName','title','date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_grams_2_prs = get_tfidf_cosine_matrix('PFR',2) #1664262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_grams_3_prs = get_tfidf_cosine_matrix('PFR',3) #5607061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_id = 59283\n",
    "top_n_most_similar(anomaly_id, tfidf_grams_2_prs, 15, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_most_similar(anomaly_id, tfidf_grams_3_prs, 15, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "# df_prs.ix[df_prs['anomalyID']==59283,['text','text_cleaned']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Unique terms:\t1041724\n",
    "Unique terms:\t3047974\n",
    "'''\n",
    "tfidf_grams_2_isa = get_tfidf_cosine_matrix('ISA',2)\n",
    "tfidf_grams_3_isa = get_tfidf_cosine_matrix('ISA',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_most_similar(59342, tfidf_isa, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_most_similar(58426, tfidf_isa, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "tfidf_grams_1_isa = get_tfidf_cosine_matrix('ISA',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISA tests for BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "58426 is a redo of 58374\n",
    "\n",
    "58426\n",
    "    58374 - 1st\n",
    "    \n",
    "58374\n",
    "    58426 - 1st\n",
    "'''\n",
    "x = top_n_most_similar(58374, tfidf_grams_2_isa, 5, False)\n",
    "print x[x['anomalyID']==58426].index[0]\n",
    "x\n",
    "\n",
    "top_n_most_similar(58426, tfidf_grams_2_isa, 5, True)\n",
    "top_n_most_similar(58374, tfidf_grams_2_isa, 5, True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "58425 is a redo of 51700 and 58793\n",
    "\n",
    "58425\n",
    "    58793 - 1st\n",
    "    51700 - 13th\n",
    "    \n",
    "51700\n",
    "    58425 - 21\n",
    "    58793 - 13\n",
    "\n",
    "58793\n",
    "    58425 - 1\n",
    "    51700 - 11\n",
    "'''\n",
    "x = top_n_most_similar(58425, tfidf_grams_2_isa, 200, False)\n",
    "x[x['anomalyID']==51700].index[0]\n",
    "\n",
    "top_n_most_similar(58425, tfidf_grams_2_isa, 13, True)\n",
    "top_n_most_similar(51700, tfidf_grams_2_isa, 21, True)\n",
    "top_n_most_similar(58793, tfidf_grams_2_isa, 11, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "52840 and 44617\n",
    "Unrelated, but similar words:  52840 and 51010\n",
    "\n",
    "52840\n",
    "    44617 - 2nd\n",
    "    51010 - 15933 th\n",
    "    \n",
    "44617\n",
    "    52840 - 61st\n",
    "    51010 - 8588th\n",
    "    \n",
    "51010\n",
    "    52840 - 21309\n",
    "    44617 - 11482\n",
    "'''\n",
    "# x = top_n_most_similar(51010, tfidf_grams_2_isa, 5, False)\n",
    "# x[x['anomalyID']==44617].index[0]\n",
    "\n",
    "#OUT\n",
    "top_n_most_similar(52840, tfidf_grams_2_isa, 15934, True)\n",
    "top_n_most_similar(44617, tfidf_grams_2_isa, 8588, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "59337, 57116 and 59246\n",
    "\n",
    "59337\n",
    "    57116 - 20th\n",
    "    59246 - 1st\n",
    "\n",
    "57116\n",
    "    59337 - 16th\n",
    "    59246 - 7th\n",
    "\n",
    "59246\n",
    "    57116 - 13th\n",
    "    59337 - 1st\n",
    "    \n",
    "Original performance\n",
    "#Related: 59337, 57116 and 59246\n",
    "x = top_n_most_similar(59337, tfidf_grams_2_isa, 400, False).reset_index() #7th and 118th\n",
    "# x = top_n_most_similar(57116, tfidf_grams_2_isa, 400, False).reset_index() #7th and 13th\n",
    "x = top_n_most_similar(59246, tfidf_grams_2_isa, 400, False).reset_index() #203rd and 8th\n",
    "'''\n",
    "# x = top_n_most_similar(59246, tfidf_grams_2_isa, 20000, False)\n",
    "# x[x['anomalyID']==59337].index[0]\n",
    "top_n_most_similar(59337, tfidf_grams_2_isa, 20, True)\n",
    "top_n_most_similar(57116, tfidf_grams_2_isa, 16, True)\n",
    "top_n_most_similar(59246, tfidf_grams_2_isa, 13, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "57007 and 49280\n",
    "\n",
    "57007\n",
    "    49280 - 1st\n",
    "    \n",
    "49280\n",
    "    57007 - 1st\n",
    "    \n",
    "Original performance\n",
    "#Related: 57007 and 49280\n",
    "x = top_n_most_similar(57007, tfidf_grams_2_isa, 400, False).reset_index() #2nd\n",
    "x = top_n_most_similar(49280, tfidf_grams_2_isa, 400, False).reset_index() #4th\n",
    "'''\n",
    "# x = top_n_most_similar(49280, tfidf_grams_2_isa, 20000, False)\n",
    "# x[x['anomalyID']==57007].index[0]\n",
    "top_n_most_similar(57007, tfidf_grams_2_isa, 5, True)\n",
    "top_n_most_similar(49280, tfidf_grams_2_isa, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring model with related documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs.ix[df_prs['anomalyID'].isin([52840,44617,59337,57116,59246,57007,49280,58426,58374,58425,51700,58793]), ['anomalyID','relatedDocuments']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "58425 is a redo of 51700 and 58793\n",
    "\n",
    "58425\n",
    "    58793 - 1st\n",
    "    51700 - 13th\n",
    "    \n",
    "51700\n",
    "    58425 - 21\n",
    "    58793 - 13\n",
    "\n",
    "58793\n",
    "    58425 - 1\n",
    "    51700 - 11\n",
    "'''\n",
    "x = top_n_most_similar(58793, tfidf_grams_2_isa, 200, False)\n",
    "x[x['anomalyID']==51700].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_prs.ix[(df_prs['reportType']=='ISA')].shape\n",
    "\n",
    "print df_prs.ix[(df_prs['reportType']=='ISA')&\n",
    "          (df_prs['relatedDocuments'].str.contains(\"(?i)ISA\")),['anomalyID','projectName','relatedDocuments']].shape\n",
    "\n",
    "print df_prs.ix[(df_prs['reportType']=='ISA')&\n",
    "          (df_prs['relatedDocuments'].str.contains(\"(?i)PFR\")),['anomalyID','projectName','relatedDocuments']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print df_prs[(df_prs['reportType']=='ISA')].shape\n",
    "print df_prs[(df_prs['reportType']=='ISA')&(~df_prs['relatedDocuments'].isnull())].shape \n",
    "print df_prs[(df_prs['reportType']=='ISA')&(df_prs['relatedDocuments'].str.contains(\"(?i)related\"))].shape \n",
    "\n",
    "df_prs.ix[(df_prs['projectName']=='Mars Science Lab')&(df_prs['reportType']=='ISA')&(~df_prs['relatedDocuments'].isnull()),['anomalyID','projectName','relatedDocuments']]\n",
    "df_prs.ix[(df_prs['projectName']=='Mars Science Lab')&\n",
    "          (df_prs['reportType']=='ISA')&\n",
    "          (df_prs['relatedDocuments'].str.contains(\"(?i)related\")),['anomalyID','projectName','relatedDocuments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs.ix[df_prs['anomalyID']==10091,[col for col in df_prs.columns if re.search('(?i)name',col)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs.ix[df_prs['title'].str.contains(\"::\"),'date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "base_path = '../Reports/Document_Similarity_Ranks/'\n",
    "df_scores = pd.DataFrame()\n",
    "for score_file in listdir(base_path):\n",
    "    df_temp = pd.read_csv(base_path+score_file)\n",
    "    df_scores = pd.concat([df_scores, df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.describe()#.plot(kind='hist',bins=100,figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.describe()#.plot(kind='hist',bins=100,figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_scores[df_scores['rank'].isnull()].shape\n",
    "print df_scores[df_scores['rank']<=20].shape\n",
    "print df_scores[df_scores['rank']>=1000].shape\n",
    "df_scores[df_scores['rank']<=10].shape[0] / float(df_scores.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[['rank']].plot(kind='hist',figsize=(15,8),bins=100)\n",
    "df_scores.ix[df_scores['rank']<=50,['rank']].plot(kind='hist',figsize=(15,8),bins=100)\n",
    "print df_scores.ix[df_scores['rank']<=10,['rank']].shape\n",
    "print df_scores.ix[df_scores['rank']>10,['rank']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.shape#.groupby('rank').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.sort_values('rank',ascending=False)\n",
    "df_scores[df_scores['rank'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[df_scores['anomaly_id']==10091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['rank'].isnull().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
