from pathlib import Path
import pickle
import re
import os

from pandas import read_csv, Series, DataFrame
import yaml

from .atm import AuthorTopicModel, Dictionary
from .reverstem import PorterStemmer
from .utility import digest_paths

# Specification description and operations

class DoctypeSpecification:
    '''
    Doctype specification generated by ProgSpecification, should not
    be instantiated without knowlege of underlying system.

    These are generated from the `config.yaml` files provided as
    constructor to ProgSpecification. These encapsulate the functions
    around use of a model specification file, including loading and
    saving variables.

    Any value added to the `config.yaml` will automatically be added,
    as a property of the object. This prevents passing strings around.
    '''
    def __init__(self, doclabel, **kwargs):
        self.doclabel = doclabel
        self.__dict__.update(kwargs)
        self.datadir = Path(self.datadir)

    @property
    def raw_anomalies(self):
        return read_csv(self.datadir / self.filename, low_memory=False)

    @property
    def attributions_table(self):
        return read_csv(self.datadir / self.attributions, low_memory=False)

    def _get_list_resource(self, label):
        with open(self.datadir / self.list_resources[label], 'r') as fd:
            targets = fd.read().splitlines()
        return targets

    @property
    def _rules(self):
        return self._get_list_resource('rules')

    @property
    def protected_words(self):
        return self._get_list_resource('protected')

    @property
    def stopwords(self):
        return self._get_list_resource('stopwords')

    @property
    def truncable_words(self):
        return self._get_list_resource('truncable')

    def FreshStemmer(self):
        return PorterStemmer(
            self.truncable_words,
            self.stopwords,
            self.protected_words,
        )

    def RegexProcessor(self):
        def rule_processor(s):
            s = ' ' + s.lower() + ' '

            for f in self._rules:
                s = re.sub(f, ' ', s)
            return ' '.join(s.split())
        return rule_processor

    def AgglomerateProcessor(self):
        def agglomerator(struct):
            return ' '.join(str(struct[label]) for label in self.free_text_labels)
        return agglomerator

    @property
    def unique_name(self):

        paths = [
            self.datadir / self.filename,
            self.datadir / self.attributions,
            self.datadir / self.experts,
        ]
        for resource in self.list_resources:
            paths.append(self.datadir / self.list_resources[resource])

        hsh = digest_paths(paths)
        return f'{self.doclabel}-{hsh}-t{self.topics}-i{self.iterations}'

    def _getdir(self, dest):
        dirname = Path(dest) / self.unique_name
        os.makedirs(dirname, exist_ok=True)
        return dirname

    def save_processed(self, dest, processed):
        dirname = self._getdir(dest)
        processed.to_csv(dirname / 'processed.csv', index=False)

    def load_processed(self, src):
        dirname = self._getdir(src)
        return read_csv(dirname / 'processed.csv', low_memory=False)

    def save_stemmer(self, dest, stemmer):
        dirname = self._getdir(dest)
        with open(dirname / 'stemmer.pickle', 'wb') as fd:
            pickle.dump(stemmer, fd)

    def load_stemmer(self, src):
        dirname = self._getdir(src)
        with open(dirname / 'stemmer.pickle', 'rb') as fd:
            stemmer = pickle.load(fd)
        return stemmer

    def save_atm(self, dest, atm):
        dirname = self._getdir(dest)
        atm.save(str(dirname / 'model.atm'))

    def load_atm(self, dest):
        dirname = self._getdir(dest)
        atm = AuthorTopicModel.load(str(dirname / 'model.atm'))
        return atm

    def save_vocab(self, dest, vocab):
        dirname = self._getdir(dest)
        vocab.save(str(dirname / 'vocab.gensim'))

    def load_vocab(self, dest):
        dirname = self._getdir(dest)
        vocab = Dictionary.load(str(dirname / 'vocab.gensim'))
        return vocab

    def _load_chunk(self, dest, label):
        dirname = self._getdir(dest)
        documents = read_csv(dirname / f'{label}_docs.csv', low_memory=False)
        attributions = read_csv(dirname / f'{label}_attrs.csv', low_memory=False)
        return documents, attributions

    def load_train(self, dest):
        return self._load_chunk(dest, 'train')

    def load_test(self, dest):
        return self._load_chunk(dest, 'test')

    def _save_chunk(self, dest, label, documents, attributions):
        dirname = self._getdir(dest)
        documents.to_csv(dirname / f'{label}_docs.csv',  index=False)

        df = DataFrame(columns=['Anomaly_ID', 'Users_ID'])
        for user in attributions:
            for anomaly_lookup in attributions[user]:
                anomaly = documents.iloc[anomaly_lookup].Anomaly_ID
                df.append({'Users_ID':user, 'Anomaly_ID': anomaly}, ignore_index=True)

        df.to_csv(dirname / f'{label}_attrs.csv', index=False)

    def save_train(self, dest, train, attr):
        self._save_chunk(dest, 'train', train, attr)

    def save_test(self, dest, test, attr):
        self._save_chunk(dest, 'test', test, attr)

    def get_expert_data(self, expert_keys):
        # takes in ordered iterable of expert_keys
        local = read_csv(
            self.datadir / self.experts,
            index_col=self.expert_key,
            low_memory=False,
        )

        return local.loc[Series(expert_keys)][self.expert_labels]

    def get_document_from_key(self, key):
        df = self.raw_anomalies
        retval = df[df[self.key] == key]
        if retval.empty:
            return {label:'' for label in self.free_text_labels}
        else:
            return retval.fillna('').iloc[0].transpose().to_dict()



class ProgSpecification(dict):
    def __init__(self, filename):
        super().__init__()
        with open(filename, 'r') as fd:
            spec_dict = yaml.load(fd)

        for key in spec_dict['document_types']:
            self[key] = DoctypeSpecification(key, **spec_dict[key])
